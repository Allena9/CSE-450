{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aUB4xrFdLkr8"
   },
   "outputs": [],
   "source": [
    "restart = True\n",
    "epoch_to_pickup = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XtiXE04uGB_U"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import contextlib\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import gc  # Import the garbage collector module\n",
    "import unicodedata\n",
    "from plyer import notification\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUgiww4oQ75T",
    "outputId": "329d5de7-4197-45e3-e5a4-76acde4a9c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Get a list of GPU devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for all GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # After enabling memory growth, we need to make sure TensorFlow sees the updated configuration\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(f\"Error setting memory growth: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "irakMtGnaImf"
   },
   "outputs": [],
   "source": [
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nDl6_okDOUyY"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# path = '/content/drive/My Drive/M6_Fall2023e/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv4r-dKnSRKz"
   },
   "source": [
    "## Functions for downloading text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xzLUaBa2Xmnb"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    text = text.replace(\"Project Gutenberg\", \"\")\n",
    "    text = text.replace(\"Gutenberg\", \"\")\n",
    "\n",
    "    # Remove carriage returns\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "\n",
    "    # fix quotes\n",
    "    text = text.replace(\"“\", \"\\\"\")\n",
    "    text = text.replace(\"”\", \"\\\"\")\n",
    "\n",
    "    # Replace any capital letter at the start of a word with ^ followed by the lowercase letter\n",
    "    text = re.sub(r\"(?<![a-zA-Z])([A-Z])\", lambda match: f\"^{match.group(0).lower()}\", text)\n",
    "\n",
    "    # Replace all other capital letters with lowercase\n",
    "    text = re.sub(r\"([A-Z])\", lambda match: f\"{match.group(0).lower()}\", text)\n",
    "\n",
    "    # Remove duplicate whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\t+\", \"\\t\", text)\n",
    "\n",
    "    # Replace whitespace characters with special words\n",
    "    text = re.sub(r\"(\\t)\", r\" zztabzz \", text)\n",
    "    text = re.sub(r\"(\\n)\", r\" zznewlinezz \", text)\n",
    "    text = re.sub(r\"(\\s)\", r\" zzspacezz \", text)\n",
    "\n",
    "    # Split before and after punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, f\" {punctuation} \")\n",
    "\n",
    "    # Makes sure that any non utf-8 characters don't make it through\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nFKehxVF9AxD"
   },
   "outputs": [],
   "source": [
    "def postprocess_text(text):\n",
    "\n",
    "    # Replace special words with whitespace characters\n",
    "    text = text.replace(\"zztabzz\", \"\\t\")\n",
    "    text = text.replace(\"zznewlinezz\", \"\\n\")\n",
    "    text = text.replace(\"zzspacezz\", \" \")\n",
    "\n",
    "    # Remake capital letters at beginning of words\n",
    "    text = re.sub(r\"\\^([a-z])\", lambda match: f\"{match.group(1).upper()}\", text)\n",
    "\n",
    "    text = text.replace(\"^\", \"\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtd9QyvUWqzi"
   },
   "source": [
    "# def getMyText():\n",
    "#   path_to_file = tf.keras.utils.get_file('austen.txt', 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt')\n",
    "\n",
    "#   text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "#   # path_to_file = tf.keras.utils.get_file('903-0.txt', 'https://www.gutenberg.org/files/903/903-0.txt')\n",
    "#   # author_text += open(path_to_file, 'rb').read().decode(encoding='utf-8')[2999:-19194]\n",
    "#   # tf.io.gfile.remove(path_to_file)\n",
    "\n",
    "#   return preprocess_text(text)\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def getMyText():\n",
    "    file_name = 'austen.txt'\n",
    "    file_url = 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt'\n",
    "    local_dir = './data/'  # Directory to save the file\n",
    "    local_path = os.path.join(local_dir, file_name)\n",
    "\n",
    "    try:\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(local_dir):\n",
    "            os.makedirs(local_dir)\n",
    "\n",
    "        # Check if the file exists locally\n",
    "        if os.path.exists(local_path):\n",
    "            print(f\"File '{file_name}' found locally. Using it.\")\n",
    "        else:\n",
    "            print(f\"File '{file_name}' not found locally. Downloading it.\")\n",
    "            # Download the file\n",
    "            downloaded_path = tf.keras.utils.get_file(file_name, file_url)\n",
    "\n",
    "            # Save the downloaded file to the designated local directory\n",
    "            with open(downloaded_path, 'rb') as source_file:\n",
    "                with open(local_path, 'wb') as dest_file:\n",
    "                    dest_file.write(source_file.read())\n",
    "\n",
    "        # Read the file's contents\n",
    "        with open(local_path, 'rb') as file:\n",
    "            text = file.read().decode(encoding='utf-8')\n",
    "\n",
    "        return preprocess_text(text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Author text import\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Number of files to save\n",
    "files = 1\n",
    "\n",
    "def getMyText(count):\n",
    "    if(count == 1):\n",
    "        file_name = 'austen.txt'\n",
    "        file_url = 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt'\n",
    "        local_dir = './data/'  # Directory to save the file\n",
    "        local_path = os.path.join(local_dir, file_name)\n",
    "\n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            if not os.path.exists(local_dir):\n",
    "                os.makedirs(local_dir)\n",
    "\n",
    "            # Check if the file exists locally\n",
    "            if os.path.exists(local_path):\n",
    "                print(f\"File '{file_name}' found locally. Using it.\")\n",
    "            else:\n",
    "                print(f\"File '{file_name}' not found locally. Downloading it.\")\n",
    "                # Download the file\n",
    "                downloaded_path = tf.keras.utils.get_file(file_name, file_url)\n",
    "\n",
    "                # Save the downloaded file to the designated local directory\n",
    "                with open(downloaded_path, 'rb') as source_file:\n",
    "                    with open(local_path, 'wb') as dest_file:\n",
    "                        dest_file.write(source_file.read())\n",
    "\n",
    "            # Read the file's contents\n",
    "            with open(local_path, 'rb') as file:\n",
    "                text = file.read().decode(encoding='utf-8')\n",
    "\n",
    "            return preprocess_text(text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "    if(count == 2):\n",
    "        file_name = 'shakespeare.txt'\n",
    "        file_url = 'https://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "        local_dir = './data/'  # Directory to save the file\n",
    "        local_path = os.path.join(local_dir, file_name)\n",
    "\n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            if not os.path.exists(local_dir):\n",
    "                os.makedirs(local_dir)\n",
    "\n",
    "            # Check if the file exists locally\n",
    "            if os.path.exists(local_path):\n",
    "                print(f\"File '{file_name}' found locally. Using it.\")\n",
    "            else:\n",
    "                print(f\"File '{file_name}' not found locally. Downloading it.\")\n",
    "                # Download the file\n",
    "                downloaded_path = tf.keras.utils.get_file(file_name, file_url)\n",
    "\n",
    "                # Save the downloaded file to the designated local directory\n",
    "                with open(downloaded_path, 'rb') as source_file:\n",
    "                    with open(local_path, 'wb') as dest_file:\n",
    "                        dest_file.write(source_file.read())\n",
    "\n",
    "            # Read the file's contents\n",
    "            with open(local_path, 'rb') as file:\n",
    "                text = file.read().decode(encoding='utf-8')\n",
    "\n",
    "            return preprocess_text(text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "    if(count == 3):\n",
    "        file_name = 'marktwain.txt'\n",
    "        file_url = 'raw.githubusercontent.com/Allena9/CSE-450/main/Rnn/data/marktwain.txt'\n",
    "        local_dir = './data/'  # Directory to save the file\n",
    "        local_path = os.path.join(local_dir, file_name)\n",
    "\n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            if not os.path.exists(local_dir):\n",
    "                os.makedirs(local_dir)\n",
    "\n",
    "            # Check if the file exists locally\n",
    "            if os.path.exists(local_path):\n",
    "                print(f\"File '{file_name}' found locally. Using it.\")\n",
    "            else:\n",
    "                print(f\"File '{file_name}' not found locally. Downloading it.\")\n",
    "                # Download the file\n",
    "                downloaded_path = tf.keras.utils.get_file(file_name, file_url)\n",
    "\n",
    "                # Save the downloaded file to the designated local directory\n",
    "                with open(downloaded_path, 'rb') as source_file:\n",
    "                    with open(local_path, 'wb') as dest_file:\n",
    "                        dest_file.write(source_file.read())\n",
    "\n",
    "            # Read the file's contents\n",
    "            with open(local_path, 'rb') as file:\n",
    "                text = file.read().decode(encoding='utf-8')\n",
    "\n",
    "            return preprocess_text(text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "rofy7hJ1iHVm",
    "outputId": "c0eaceda-482d-47a6-c54a-10c40d7aeaea"
   },
   "source": [
    "getMyText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gsCd-ihOU02C"
   },
   "outputs": [],
   "source": [
    "def getRandomText(numbooks = 1, verbose=False):\n",
    "  # Create a buffer to capture download output messages\n",
    "  download_log = io.StringIO()\n",
    "  # Initialize empty string to store text from books\n",
    "  text_random = ''\n",
    "  # Loop through the requested number of books\n",
    "  for b in range(numbooks):\n",
    "    # Flag to track whether we've found a suitable book\n",
    "    foundbook = False\n",
    "    # Keep trying until we find a valid book\n",
    "    while(foundbook == False):\n",
    "      # Generate random book ID between 100-60000\n",
    "      booknum = random.randint(100,60000)\n",
    "      if verbose:\n",
    "        print('Trying Book #: ',booknum)\n",
    "      # Randomly choose between two URL formats for Project Gutenberg\n",
    "      if random.random() > 0.5:\n",
    "        url = 'https://www.gutenberg.org/files/' + str(booknum) + '/' + str(booknum) + '-0.txt'\n",
    "        filename_temp = str(booknum) + '-0.txt'\n",
    "      else:\n",
    "        url = 'https://www.gutenberg.org/cache/epub/' + str(booknum) + '/pg' + str(booknum) + '.txt'\n",
    "        filename_temp = 'pg' + str(booknum) + '.txt'\n",
    "      if verbose:\n",
    "        print('Trying: ', url)\n",
    "      try:\n",
    "        # Download the file, either showing progress or hiding it based on verbose flag\n",
    "        if verbose:\n",
    "          path_to_file_temp = tf.keras.utils.get_file(filename_temp, url)\n",
    "        else:\n",
    "          with contextlib.redirect_stdout(download_log):\n",
    "            path_to_file_temp = tf.keras.utils.get_file(filename_temp, url)\n",
    "        # Read the downloaded file and decode as UTF-8\n",
    "        temptext = open(path_to_file_temp, 'rb').read().decode(encoding='utf-8')\n",
    "        # Delete the temporary file after reading its contents\n",
    "        tf.io.gfile.remove(path_to_file_temp)\n",
    "        # Check if the book is in English\n",
    "        if (temptext.find('Language: English') >= 0):\n",
    "          # Add slight randomness to the starting position\n",
    "          offset = random.randint(-20,20)\n",
    "          # Skip the header/preamble text (typically first 2000 chars)\n",
    "          header = 2000\n",
    "          # Target length for extracted text\n",
    "          total_length = 200000\n",
    "          # Amount to trim from the end of the book\n",
    "          chopoffend = 10000\n",
    "          # For long books: extract a fixed-size chunk\n",
    "          if len(temptext) > (header+total_length+offset+chopoffend):\n",
    "            foundbook = True\n",
    "            text_random += temptext[header+offset:header+total_length+offset]\n",
    "            #print(\"Yes: \" + str(booknum))\n",
    "            if verbose:\n",
    "              print('New size of dataset: ', len(text_random))\n",
    "          # For medium-length books: take most of the book minus header and footer\n",
    "          elif len(temptext) > (header+12000):\n",
    "            foundbook = True\n",
    "            text_random += temptext[header:-chopoffend]\n",
    "            #print(\"Yes (smaller): \" + str(booknum))\n",
    "            if verbose:\n",
    "              print('New size of dataset: ', len(text_random))\n",
    "          # Skip books that are too short\n",
    "          else:\n",
    "            if verbose:\n",
    "              print('Not long enough. Trying again...')\n",
    "            #print(\"No: \" + str(booknum) + \" too short\")\n",
    "        # Skip non-English books\n",
    "        else:\n",
    "          if verbose:\n",
    "            print('Not English. Trying again...')\n",
    "          #print(\"No: \" + str(booknum) + \" not English\")\n",
    "        # Clean up memory\n",
    "        del temptext\n",
    "      # Handle any exceptions during download/processing\n",
    "      except:\n",
    "        if verbose:\n",
    "          print('Not valid file. Trying again...')\n",
    "        #print(\"No: \" + str(booknum) + \" not valid\")\n",
    "        foundbook = False\n",
    "    if verbose:\n",
    "      print(\"Found \" + str(b+1) + \" books so far...\")\n",
    "  # Clean up memory\n",
    "  del download_log\n",
    "  # The following commented code appears to be for further processing that was replaced\n",
    "  #text_random = \"\".join(c for c in text_random if c in vocab)\n",
    "  #all_ids_random = ids_from_chars(tf.strings.unicode_split(text_random, 'UTF-8'))\n",
    "  #ids_dataset_random = tf.data.Dataset.from_tensor_slices(all_ids_random)\n",
    "  #sequences_random = ids_dataset_random.batch(seq_length+1, drop_remainder=True)\n",
    "  #dataset_random = sequences_random.map(split_input_target)\n",
    "  #dataset_random = (dataset_random.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "  #return dataset_random\n",
    "  \n",
    "  # Pass the collected text to a preprocessing function and return the result\n",
    "  return preprocess_text(text_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjEF0LKxhljS",
    "outputId": "c0e93e56-0db7-40b3-db3c-1aae811aed42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'austen.txt' found locally. Using it.\n"
     ]
    }
   ],
   "source": [
    "if restart:\n",
    "  vocab_text = getMyText(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpFvtyF_g3jY"
   },
   "source": [
    "Make vocabulary (Adapted from TensorFlow word embedding tutorial)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "F8E6Q6dkMEpd"
   },
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 12288\n",
    "sequence_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AWXUqLQ6g3KB"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_string):\n",
    "    # Lowercase the text\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    \n",
    "    # Handle common punctuation marks one by one\n",
    "    # This avoids regex escaping issues\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'\\.', ' . ')  # Period\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r',', ' , ')   # Comma\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'!', ' ! ')   # Exclamation\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'\\?', ' ? ')  # Question mark\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r';', ' ; ')   # Semicolon\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r':', ' : ')   # Colon\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'\\(', ' ( ')  # Open parenthesis\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'\\)', ' ) ')  # Close parenthesis\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'\\[', ' [ ')  # Open bracket\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'\\]', ' ] ')  # Close bracket\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'\\{', ' { ')  # Open brace\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'\\}', ' } ')  # Close brace\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'\"', ' \" ')   # Double quote\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r\"'\", \" ' \")   # Single quote\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'-', ' - ')   # Hyphen\n",
    "    lowercase = tf.strings.regex_replace(lowercase, r'\\*', '')   # Hyphen\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    return tf.strings.regex_replace(lowercase, r'\\s+', ' ')\n",
    "\n",
    "if restart:\n",
    "  # Use the text vectorization layer to normalize, split, and map strings to\n",
    "  # integers. Note that the layer uses the custom standardization defined above.\n",
    "  # Set maximum_sequence length as all samples are not of the same length.\n",
    "  vectorize_layer = TextVectorization(\n",
    "      standardize=custom_standardization,\n",
    "      split='whitespace',\n",
    "      max_tokens=vocab_size,\n",
    "      output_mode='int',\n",
    "      #output_sequence_length=sequence_length\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zJfr5w1bTWiJ"
   },
   "outputs": [],
   "source": [
    "if restart:\n",
    "  # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "  vectorize_layer.adapt([vocab_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PmaoiyvF1Ilm"
   },
   "outputs": [],
   "source": [
    "if restart:\n",
    "  vocabulary = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7ULNtM_8nYn"
   },
   "source": [
    "Save Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "G1hjxv447INt"
   },
   "outputs": [],
   "source": [
    "if restart:\n",
    "  with open(path + \"vocabulary.txt\", \"w\") as file:\n",
    "    for word in vocabulary:\n",
    "        file.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7qn5MjC8p0_"
   },
   "source": [
    "Load Saved Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TLbSoqUP8Pxu"
   },
   "outputs": [],
   "source": [
    "if restart == False:\n",
    "  with open(path + \"vocabulary.txt\", \"r\") as file:\n",
    "      vocabulary = [word.strip() for word in file.readlines()]\n",
    "      vocabulary = vocabulary\n",
    "\n",
    "  vectorize_layer = TextVectorization(\n",
    "      vocabulary=vocabulary,\n",
    "      standardize='lower',\n",
    "      split='whitespace',\n",
    "      max_tokens=vocab_size,\n",
    "      output_mode='int',\n",
    "      #output_sequence_length=sequence_length\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FidGlurb1iD3",
    "outputId": "50b05e9e-68b7-4cda-a427-b879a3e3a5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'zzspacezz', '^', ',', '.', 'the', 'to', 'and', 'of', '\"', 'a', 'her', '-', 'i', 'was', 'in', 'it', 'she', ';']\n",
      "['jenkinsons', 'jeffereys', 'jar', 'jades', 'jackonet', 'jacket', 'ivory', 'islands', 'irritating', 'irritates', 'irritability', 'irrevocably', 'irretrievable', 'irresistibly', 'irreparable', 'irreligious', 'irregularity', 'irrecoverably', 'ire', 'inward']\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary[:20])\n",
    "print(vocabulary[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LovypAGk91Yp"
   },
   "source": [
    "Turn text into a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Mnp0huUX93Wi"
   },
   "outputs": [],
   "source": [
    "# This function will generate our sequence pairs:\n",
    "def split_input_target(sequence):\n",
    "    input_ids = sequence[:-1]\n",
    "    target_ids = sequence[1:]\n",
    "    return input_ids, target_ids\n",
    "\n",
    "# This function will create the dataset\n",
    "def text_to_dataset(text):\n",
    "  all_ids = vectorize_layer(text)\n",
    "  ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "  del all_ids\n",
    "  sequences = ids_dataset.batch(sequence_length+1, drop_remainder=True)\n",
    "  del ids_dataset\n",
    "\n",
    "  # Call the function for every sequence in our list to create a new dataset\n",
    "  # of input->target pairs\n",
    "  dataset = sequences.map(split_input_target)\n",
    "  del sequences\n",
    "\n",
    "  # shuffle\n",
    "\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afRybxef_QHi"
   },
   "source": [
    "Test on vocab text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0tBa6ttN_Ufz"
   },
   "outputs": [],
   "source": [
    "if restart:\n",
    "  vocab_ds = text_to_dataset(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vq191mRgWv2w"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  text = ''.join([vocabulary[index] for index in ids])\n",
    "  return postprocess_text(text)\n",
    "\n",
    "vocabulary_adjusted = vocabulary\n",
    "vocabulary_adjusted[0] = '[UNK]'\n",
    "vocabulary_adjusted[1] = ''\n",
    "\n",
    "words_from_ids = StringLookup(vocabulary=vocabulary_adjusted, invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDqaTHXFAEBD",
    "outputId": "7dc76f61-3d65-4d2a-c8ca-6ff765949c24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "tf.Tensor(\n",
      "[   3 2059    2    3   14    2    3  353    2    3   14    2    3  155\n",
      "    2    3  401    4    2  557    4    2 1075    4    2    8    2 1016\n",
      "    4    2   29    2   11    2  629    2  174    2    8    2  187    2\n",
      "  652    4    2  183    2    7    2 3688    2   98    2    9    2    6\n",
      "    2  271    2 3512    2    9    2 1802   19    2    8    2   24    2\n",
      "  815    2  899    2  636   13   75    2  340    2   16    2    6    2\n",
      "  241    2   29    2   36    2   96    2    7    2  709    2   57    2\n",
      " 4221    2   12    5    2    3   18    2   15    2    6    2 1758    2\n",
      "    9    2    6    2  122    2  548    2    9    2   11    2  108    2\n",
      "  943    4], shape=(128,), dtype=int64)\n",
      "Volume I Chapter I Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her. She was the youngest of the two daughters of a most affectionate,\n",
      "tf.Tensor(\n",
      "[b'^' b'volume' b'zzspacezz' b'^' b'i' b'zzspacezz' b'^' b'chapter'\n",
      " b'zzspacezz' b'^' b'i' b'zzspacezz' b'^' b'emma' b'zzspacezz' b'^'\n",
      " b'woodhouse' b',' b'zzspacezz' b'handsome' b',' b'zzspacezz' b'clever'\n",
      " b',' b'zzspacezz' b'and' b'zzspacezz' b'rich' b',' b'zzspacezz' b'with'\n",
      " b'zzspacezz' b'a' b'zzspacezz' b'comfortable' b'zzspacezz' b'home'\n",
      " b'zzspacezz' b'and' b'zzspacezz' b'happy' b'zzspacezz' b'disposition'\n",
      " b',' b'zzspacezz' b'seemed' b'zzspacezz' b'to' b'zzspacezz' b'unite'\n",
      " b'zzspacezz' b'some' b'zzspacezz' b'of' b'zzspacezz' b'the' b'zzspacezz'\n",
      " b'best' b'zzspacezz' b'blessings' b'zzspacezz' b'of' b'zzspacezz'\n",
      " b'existence' b';' b'zzspacezz' b'and' b'zzspacezz' b'had' b'zzspacezz'\n",
      " b'lived' b'zzspacezz' b'nearly' b'zzspacezz' b'twenty' b'-' b'one'\n",
      " b'zzspacezz' b'years' b'zzspacezz' b'in' b'zzspacezz' b'the' b'zzspacezz'\n",
      " b'world' b'zzspacezz' b'with' b'zzspacezz' b'very' b'zzspacezz' b'little'\n",
      " b'zzspacezz' b'to' b'zzspacezz' b'distress' b'zzspacezz' b'or'\n",
      " b'zzspacezz' b'vex' b'zzspacezz' b'her' b'.' b'zzspacezz' b'^' b'she'\n",
      " b'zzspacezz' b'was' b'zzspacezz' b'the' b'zzspacezz' b'youngest'\n",
      " b'zzspacezz' b'of' b'zzspacezz' b'the' b'zzspacezz' b'two' b'zzspacezz'\n",
      " b'daughters' b'zzspacezz' b'of' b'zzspacezz' b'a' b'zzspacezz' b'most'\n",
      " b'zzspacezz' b'affectionate' b','], shape=(128,), dtype=string)\n",
      "Target: \n",
      "tf.Tensor(\n",
      "[2059    2    3   14    2    3  353    2    3   14    2    3  155    2\n",
      "    3  401    4    2  557    4    2 1075    4    2    8    2 1016    4\n",
      "    2   29    2   11    2  629    2  174    2    8    2  187    2  652\n",
      "    4    2  183    2    7    2 3688    2   98    2    9    2    6    2\n",
      "  271    2 3512    2    9    2 1802   19    2    8    2   24    2  815\n",
      "    2  899    2  636   13   75    2  340    2   16    2    6    2  241\n",
      "    2   29    2   36    2   96    2    7    2  709    2   57    2 4221\n",
      "    2   12    5    2    3   18    2   15    2    6    2 1758    2    9\n",
      "    2    6    2  122    2  548    2    9    2   11    2  108    2  943\n",
      "    4    2], shape=(128,), dtype=int64)\n",
      "volume I Chapter I Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her. She was the youngest of the two daughters of a most affectionate, \n"
     ]
    }
   ],
   "source": [
    "if restart:\n",
    "  for input_example, target_example in vocab_ds.take(1):\n",
    "    print(\"Input: \")\n",
    "    print(input_example)\n",
    "    print(text_from_ids(input_example))\n",
    "    print(words_from_ids(input_example))\n",
    "    print(\"Target: \")\n",
    "    print(target_example)\n",
    "    print(text_from_ids(target_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Rp402vgrS54t"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "def setup_dataset(dataset):\n",
    "  dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0LdoMfT7T8WN"
   },
   "outputs": [],
   "source": [
    "if restart:\n",
    "  vocab_ds = setup_dataset(vocab_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VQ-KjEeZMzd"
   },
   "source": [
    "## III. Build the model\n",
    "\n",
    "Next, we'll build our model. Up until this point, you've been using the Keras symbolic, or imperative API for creating your models. Doing something like:\n",
    "\n",
    "    model = tf.keras.models.Sequentla()\n",
    "    model.add(tf.keras.layers.Dense(80, activation='relu))\n",
    "    etc...\n",
    "\n",
    "However, tensorflow has another way to build models called the Functional API, which gives us a lot more control over what happens inside the model. You can read more about [the differences and when to use each here](https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html).\n",
    "\n",
    "We'll use the functional API for our RNN in this example. This will involve defining our model as a custom subclass of `tf.keras.Model`.\n",
    "\n",
    "If you're not familiar with classes in python, you might want to review [this quick tutorial](https://www.w3schools.com/python/python_classes.asp), as well as [this one on class inheritance](https://www.w3schools.com/python/python_inheritance.asp).\n",
    "\n",
    "Using a functional model is important for our situation because we're not just training it to predict a single character for a single sequence, but as we make predictions with it, we need it to remember those predictions as use that memory as it makes new predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Fj4uh9y-Y9mx"
   },
   "outputs": [],
   "source": [
    "# Create our custom model. Given a sequence of characters, this\n",
    "# model's job is to predict what character should come next.\n",
    "class AustenTextModel(tf.keras.Model):\n",
    "\n",
    "  # This is our class constructor method, it will be executed when\n",
    "  # we first create an instance of the class\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__()\n",
    "\n",
    "    # Our model will have three layers:\n",
    "\n",
    "    # 1. An embedding layer that handles the encoding of our vocabulary into\n",
    "    #    a vector of values suitable for a neural network\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    # 2. A GRU layer that handles the \"memory\" aspects of our RNN. If you're\n",
    "    #    wondering why we use GRU instead of LSTM, and whether LSTM is better,\n",
    "    #    take a look at this article: https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm\n",
    "    #    then consider trying out LSTM instead (or in addition to!)\n",
    "    #self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "    self.lstm1 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
    "    self.lstm2 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
    "    self.lstm3 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
    "    # self.lstm4 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
    "\n",
    "\n",
    "    self.hidden1 = tf.keras.layers.Dense(embedding_dim*64, activation='relu')\n",
    "    self.hidden2 = tf.keras.layers.Dense(embedding_dim*16, activation='relu')\n",
    "    #self.hidden3 = tf.keras.layers.Dense(embedding_dim*4, activation='relu')\n",
    "\n",
    "    # 3. Our output layer that will give us a set of probabilities for each\n",
    "    #    character in our vocabulary.\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  # This function will be executed for each epoch of our training. Here\n",
    "  # we will manually feed information from one layer of our network to the\n",
    "  # next.\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "\n",
    "    # 1. Feed the inputs into the embedding layer, and tell it if we are\n",
    "    #    training or predicting\n",
    "    x = self.embedding(x, training=training)\n",
    "\n",
    "    # 2. If we don't have any state in memory yet, get the initial random state\n",
    "    #    from our GRUI layer.\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    if states is None:\n",
    "      states1 = [tf.zeros([batch_size, self.lstm1.units]), tf.zeros([batch_size, self.lstm1.units])]\n",
    "      states2 = [tf.zeros([batch_size, self.lstm2.units]), tf.zeros([batch_size, self.lstm2.units])]\n",
    "      states3 = [tf.zeros([batch_size, self.lstm3.units]), tf.zeros([batch_size, self.lstm3.units])]\n",
    "      # states4 = [tf.zeros([batch_size, self.lstm4.units]), tf.zeros([batch_size, self.lstm4.units])]\n",
    "    else:\n",
    "      states1 = states[0]\n",
    "      states2 = states[1]\n",
    "      states3 = states[2]\n",
    "      # states4 = states[3]\n",
    "    # 3. Now, feed the vectorized input along with the current state of memory\n",
    "    #    into the gru layer.\n",
    "    x, state_h_1, state_c_1 = self.lstm1(x, initial_state=states1, training=training)\n",
    "    states_out_1 = [state_h_1,state_c_1]\n",
    "\n",
    "    x, state_h_2, state_c_2 = self.lstm2(x, initial_state=states2, training=training)\n",
    "    states_out_2 = [state_h_2,state_c_2]\n",
    "\n",
    "    x, state_h_3, state_c_3 = self.lstm3(x, initial_state=states3, training=training)\n",
    "    states_out_3 = [state_h_3,state_c_3]\n",
    "\n",
    "    # x, state_h_4, state_c_4 = self.lstm4(x, initial_state=states4, training=training)\n",
    "    # states_out_4 = [state_h_4,state_c_4]\n",
    "\n",
    "    states_out = [states_out_1, states_out_2, states_out_3] #, states_out_4]\n",
    "    #states_out = [states_out_1, states_out_2]\n",
    "\n",
    "    x = self.hidden1(x,training=training)\n",
    "    x = self.hidden2(x,training=training)\n",
    "    #x = self.hidden3(x,training=training)\n",
    "    # 4. Finally, pass the results on to the dense layer\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    # 5. Return the results\n",
    "    if return_state:\n",
    "      return x, states_out\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NGm9o_J8Tq2F"
   },
   "outputs": [],
   "source": [
    "if restart:\n",
    "  dataset = vocab_ds\n",
    "  del vocab_text\n",
    "  del vocab_ds\n",
    "else:\n",
    "  new_text = getRandomText(numbooks = 10)\n",
    "  dataset = text_to_dataset(new_text)\n",
    "  del new_text\n",
    "  dataset = setup_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "UA2C6pxZc4De"
   },
   "outputs": [],
   "source": [
    "# Create an instance of our model\n",
    "#vocab_size=len(ids_from_chars.get_vocabulary())\n",
    "embedding_dim = 128\n",
    "rnn_units = 512\n",
    "\n",
    "model = AustenTextModel(vocab_size, embedding_dim, rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C67kN7YAdfSf",
    "outputId": "87af41fd-6cbe-4d7b-9ff9-2cdce14a280d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 12288) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# Verify the output of our model is correct by running one sample through\n",
    "# This will also compile the model for us. This step will take a bit.\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "qJGL8gCWdsiu",
    "outputId": "fe8a8959-c379-4369-83dc-22dba8d4e3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"austen_text_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1572864   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  1312768   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  2099200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  4202496   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  16779264  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  25178112  \n",
      "=================================================================\n",
      "Total params: 53,243,904\n",
      "Trainable params: 53,243,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now let's view the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "UDbtrI9tc2NH"
   },
   "outputs": [],
   "source": [
    "# Here's the code we'll use to sample for us. It has some extra steps to apply\n",
    "# the temperature to the distribution, and to make sure we don't get empty\n",
    "# characters in our text. Most importantly, it will keep track of our model\n",
    "# state for us.\n",
    "\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, vectorize_layer, vocabulary, temperature=1):\n",
    "    super().__init__()\n",
    "    self.temperature=temperature\n",
    "    self.model = model\n",
    "    self.vectorize_layer = vectorize_layer\n",
    "    self.vocabulary = vocabulary\n",
    "    #print(\"initialized\")\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "    skip_ids = StringLookup(vocabulary=list(vocabulary))(['', '[UNK]'])[:, None]\n",
    "    #print(skip_ids)\n",
    "    #print(\"3\")\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices = skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(vocabulary)])\n",
    "    #print(\"4\")\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
    "    #print(\"5\")\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    #input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.vectorize_layer(inputs)\n",
    "    #print(input_ids)\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    del input_ids\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "\n",
    "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    del predicted_logits\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    #print(predicted_ids[0])\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return words_from_ids(predicted_ids), states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "P3WQoFaE7Ol2"
   },
   "outputs": [],
   "source": [
    "def produce_sample(model, vectorize_layer, vocabulary, temp, epoch, prompt):\n",
    "  # Create an instance of the character generator\n",
    "  #print(\"entered\")\n",
    "  one_step_model = OneStep(model, vectorize_layer, vocabulary, temp)\n",
    "  #print(\"rand one step\")\n",
    "  # Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
    "  # as its starting text\n",
    "  states = None\n",
    "  next_char = tf.constant([preprocess_text(prompt)])\n",
    "  result = [tf.constant([prompt])]\n",
    "\n",
    "  for n in range(200):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    #print(next_char)\n",
    "    result.append(next_char)\n",
    "    #print(result)\n",
    "\n",
    "  result = tf.strings.join(result)\n",
    "  #print(result)\n",
    "\n",
    "  # Print the results formatted.\n",
    "  #print('Temp: ' + str(temp) + '\\n')\n",
    "  print(postprocess_text(result[0].numpy().decode('utf-8')))\n",
    "  #print('\\n\\n')\n",
    "  print('Epoch: ' + str(epoch) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
    "  print('Temp: ' + str(temp) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
    "  print(postprocess_text(result[0].numpy().decode('utf-8')), file=open(path + 'tree.txt', 'a'))\n",
    "  print('\\n\\n', file=open(path + 'tree.txt', 'a'))\n",
    "  del states\n",
    "  del next_char\n",
    "  del result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTDe5m4baEqo"
   },
   "source": [
    "## IV. Train the model\n",
    "\n",
    "For our purposes, we'll be using [categorical cross entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) as our loss function*. Also, our model will be outputting [\"logits\" rather than normalized probabilities](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow), because we'll be doing further transformations on the output later.\n",
    "\n",
    "\n",
    "\\* Note that since our model deals with integer encoding rather than one-hot encoding, we'll specifically be using [sparse categorical cross entropy](https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "mOP5s0SmIhUO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'austen.txt' found locally. Using it.\n"
     ]
    }
   ],
   "source": [
    "sherlock_text = getMyText(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSk7HBJe_RZi"
   },
   "source": [
    "if restart == False:\n",
    "  model.load_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vOxc7CkaGQB",
    "outputId": "5524a5d1-2723-421e-9d05-de13ff75ab0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "152/152 [==============================] - 18s 101ms/step - loss: 4.2848\n",
      "finished training...\n",
      "epoch:  1\n",
      "147/147 [==============================] - 16s 101ms/step - loss: 2.8336\n",
      "finished training...\n",
      "epoch:  2\n",
      "158/158 [==============================] - 17s 101ms/step - loss: 2.6672\n",
      "finished training...\n",
      "epoch:  3\n",
      "148/148 [==============================] - 16s 101ms/step - loss: 2.4847\n",
      "finished training...\n",
      "epoch:  4\n",
      "169/169 [==============================] - 18s 102ms/step - loss: 2.4933\n",
      "finished training...\n",
      "epoch:  5\n",
      "141/141 [==============================] - 15s 103ms/step - loss: 2.4271\n",
      "finished training...\n",
      "epoch:  6\n",
      "158/158 [==============================] - 17s 102ms/step - loss: 2.2908\n",
      "finished training...\n",
      "epoch:  7\n",
      "167/167 [==============================] - 18s 102ms/step - loss: 2.3983\n",
      "finished training...\n",
      "epoch:  8\n",
      "159/159 [==============================] - 17s 102ms/step - loss: 2.3779\n",
      "finished training...\n",
      "epoch:  9\n",
      "153/153 [==============================] - 16s 102ms/step - loss: 2.3024\n",
      "finished training...\n",
      "epoch:  10\n",
      "151/151 [==============================] - 16s 102ms/step - loss: 2.2282\n",
      "finished training...\n",
      "epoch:  11\n",
      "132/132 [==============================] - 14s 102ms/step - loss: 2.2219\n",
      "finished training...\n",
      "epoch:  12\n",
      "155/155 [==============================] - 17s 102ms/step - loss: 2.3194\n",
      "finished training...\n",
      "epoch:  13\n",
      "171/171 [==============================] - 18s 102ms/step - loss: 2.2572\n",
      "finished training...\n",
      "epoch:  14\n",
      "160/160 [==============================] - 17s 102ms/step - loss: 2.2415\n",
      "finished training...\n",
      "epoch:  15\n",
      "151/151 [==============================] - 16s 102ms/step - loss: 2.1895\n",
      "finished training...\n",
      "epoch:  16\n",
      "150/150 [==============================] - 16s 102ms/step - loss: 2.1827\n",
      "finished training...\n",
      "epoch:  17\n",
      "161/161 [==============================] - 17s 102ms/step - loss: 2.1672\n",
      "finished training...\n",
      "epoch:  18\n",
      "157/157 [==============================] - 17s 102ms/step - loss: 2.1770\n",
      "finished training...\n",
      "epoch:  19\n",
      "143/143 [==============================] - 15s 102ms/step - loss: 2.0803\n",
      "finished training...\n",
      "epoch:  20\n",
      "254/254 [==============================] - 26s 102ms/step - loss: 2.2167\n",
      "finished training...\n",
      "epoch:  21\n",
      "245/245 [==============================] - 26s 103ms/step - loss: 2.0351\n",
      "finished training...\n",
      "Emma sat thinking about her with her usual looks of pleasure. He had known her to be a very good-tempered girl, and Mrs. Norris was very much surprised that he should be so much attached to her sister, and to have her own way to London. His business was not so much counteracted by the Middletons, and Mrs. Norris had been most happy in thinking of Mr. Darcy, and had been able to give them all the trouble of making her the honour of being settled in the next morning. She had \n",
      "Emma sat thinking about her with all the eagerness of his two sisters with her sister, and the two ladies were all together together, and Mrs. Norris was again in the midst of the rest. Chapter V Mr. Darcy was on the point of being a very handsome young man; and Marianne, who was now in her own room, had been glad to see her again to be seen again. They were all very much disposed to be in the room, and Mrs. Norris was almost as much recovered as she could wish \n",
      "Emma sat thinking about Mrs. Jennings and Mary a very pretty girl, which would be her welcome to her mother, and he was now in a state of ill-humour and pleasure. Mrs. Norris was gone with her mother, and Mr. Knightley was at the same time of her being in Devonshire again with Mr. Rushworths letter. He was two times a little longer, and they all was always afraid of being useful to her. She found her a great deal of happy scenes for herself to be determined to call in \n",
      "Emma sat thinking about it, in short, what she had supposed to do she ever saw in Fanny; and with the most of the other, to return only the change of her own family, it was evident how to be assured of her to marry her own, that Mrs. Gardiner gave her a serious negative; and she had been glad to write to her in the attitude of her sisters, which she had hoped to think it possible to make her the most pleasing of the two sisters, which she could not avoid no more of the \n",
      "Emma sat thinking about him. At length the others entered. \"My dear,\" said he, \"but it is not a surprise to Miss Dashwood to be satisfied. But he did not think it a very shining young man, and did not mean to find myself. As to the Baron, it is a great one, and you are all here in Kent now.\" \"Oh! no; he is not coming to Portsmouth, I am sure, Miss Woodhouse. Mrs. Allen, to be in London, must be given in \n",
      "Emma sat thinking about as they drove in. She was now to beg his opinion of his conversation--and how much did Mary dance and say only that she was rather beyond any inconvenience on her side; and she found a few hours before she was at Fullerton. Elizabeth was still standing by Edmund about Mrs. Goddards, in which curiosity and delicacy had the satisfaction of both--but from Lady Russell's darling opinion? That is all very pretty! The happiness of her staying at Netherfield (for some twenty minutes after she should\n",
      "samples produced...\n",
      "garbage collected...\n",
      "session cleared (to save memory)...\n",
      "epoch:  22\n",
      "245/245 [==============================] - 26s 102ms/step - loss: 1.9035\n",
      "finished training...\n",
      "Emma sat thinking about her sister; and she was not so much pleased with Miss Crawford as to the Miss Steeles. Chapter 31 The next day produced a very favourable communication of Sir Thomas. The Allens were not in the power of a subject which he had been previously speaking of as his sister, and of his being only as much attached to Mr. Bingley than his sister. The Admiral and Mrs. Grant were to go to London, and be able to go on horseback by her husband. They were \n",
      "Emma sat thinking about her sister's mind to think that she was of no use, and she had already received a letter from her aunt Norris to bring herself to the point again. As she was not so diffident of her own importance, she could not be afraid of her affection to go early and change her own comfort. The delight of Harriets being gone to Cleveland was a favourable object to her to be loved by any other sister in the world. She was very much in love with Mr. Crawford and Miss Bingley\n",
      "Emma sat thinking about Mr. Weston was to enter into the room without feeling the feelings of his own mind in the opinion of himself. She had so much to do with any other feeling of his feelings, and to lessen her anxiety on the subject, and with a little curiosity at the idea of her own unhappiness, she was not received from the unhappy conviction of her being in love with her sister, in a moment to be in love with her. Every thing was her object of being perfectly right. She had no reason to \n",
      "Emma sat thinking about the Eltons, who had before reached Fullerton, and never had her in her life been so far from partiality for her sister as she answered. Chapter 24 The Honourable Miss Crawford, of Elinor's visit to Longbourn was the last to be of them. Captain Benwick, Miss Crawford, and Mr. Crawford, were to set off by his son to Mrs. John Dashwood, a very respectable widow, who, in the interval of Mrs. Ferrars's last meeting, had been some time in the \n",
      "Emma sat thinking about her own amiable mother-in-law. Chapter 19 Sir Thomas Thorpe seemed to have allowed herself to be called into a communication to be the object of a Lady Middleton, and when the servant had written to Mr. Bennet, he came to the Park, and the change was the Harvilles settled, and Sir Walter had entered into felicity. Mr. Collins seemed as agreeable in the affair as being one of the party sitting down together in such a manner as his mother, and nothing better that in the \n",
      "Emma sat thinking about them whenever the news was dressing-work; but if Mrs. Bennet, Mr. Darcy, talk to and sing as much as possible. The whole evening was certainly the first consolation. But Jane, however, was in a fairer humour for the evening; and Isabella, with eyes full of preparation, went out of the room by her eager observation to be with her; and though everything was impossible had the perceiving long of this, there were neither time nor to nor to all this: it was all the grand friend in \n",
      "samples produced...\n",
      "garbage collected...\n",
      "session cleared (to save memory)...\n",
      "epoch:  23\n",
      "245/245 [==============================] - 26s 104ms/step - loss: 1.7705\n",
      "finished training...\n",
      "Emma sat thinking about her own party, and was almost as eager to have her own children to give her the offer of a Mr. Elton. She had not a doubt of its being his own concerns. She had been too much engaged to answer the possibility of a whole fortnight to be the object of her happiness. She had a great deal of attention to her daughter, and the object of her behaviour to her sister, and her sister to Willoughby was to be the favourite of Mr. Darcy, and her own consequence of \n",
      "Emma sat thinking about her arrival in town, and she had been in company with her, and Mr. Rushworth was at the age of seventeen to come in to the Laconia. The Gardiners were going to walk with them to Meryton, and Mrs. Norris was invited to join her in the same way, and Mary had been in the room again by the entrance of Frank Churchill. Mr. Crawford was often called out of Mansfield Park. The Miss Bertrams were more than licensed to be met, and the young \n",
      "Emma sat thinking about her for a few minutes. She was growing obliged to go. \"I am sure I shall not have thought I had any thing to say of you yesterday. I am sure I never saw a man so extremely pretty at my uncles, when you have had a comfortable walk in London. I am sure I was so kind to have you so very obliging to see him.\" Fanny was no longer astonished at the idea, but not chusing she hoped it would be done with him the next morning, and\n",
      "Emma sat thinking about it all from that time, and as her own heart might have been insensible of the fact of her mother to be important as to all the others, she had found herself applied to her thoughts, and here she was her sister in another minute and then she was left to see her sister. They were too much at home. It was a very long time, the whole six were very lively, and as it appeared that the Miss Steeles was in company with an old man of sense, with a very serious look\n",
      "Emma sat thinking about it all by her cousin. \"Dear Jane,\" said she, \"I shall be here too. Rushworth is come to settle nothing else.\" \"You know I am not going away to say so; and I am sure I shall keep my commission.\" \"I never saw him so altered there as nearly ever she can do. He is as lively as her brother she and I was at home. She is quite sure that every body else should be distressed for summer together.\" \"As soon as you\n",
      "Emma sat thinking about it as to the utmost. \"To mix among her, however, ask me how she is so.\" \"Nobody else; that is, you always know what she means to have, and beg some engagement with me as he is.\" \"But it is a very serious one,\" replied Elizabeth. \"There is only Miss Lucy. I have the charge of Miss Elliot; and in such a manner as must have seen her of me. She has been kind to him, to be sure. She brightened up. \n",
      "samples produced...\n",
      "garbage collected...\n",
      "session cleared (to save memory)...\n",
      "epoch:  24\n",
      "245/245 [==============================] - 26s 103ms/step - loss: 1.6267\n",
      "finished training...\n",
      "Emma sat thinking about Captain Wentworth at all hours of her fathers health, and she was so kindly asked to see her so much in the same room as she had ever known before. She could not have the smallest idea of it. She was in a state of spirits, when she considered how much unsuspected was her behaviour to her. She had heard nothing of it till Mary's leaving London; and she had no opportunity of answering the subject, and was beginning to feel herself sensible of the possibility of his coming there. It\n",
      "Emma sat thinking about her, and Elinor could not help feeling that she had never been able to speak plainer with the utmost politeness. She saw them instantly with a look of horror and a little subdued admiration; but she was not so much interested in Marianne as to be in a way unsuspected by Lady Russell and Anne. Her mind was in a flutter of spirits, and she was the only one of the other close from the carriage. The two gentlemen, though not remarkably good enough, were immediately recovering his sight. She was\n",
      "Emma sat thinking about her husband in her way to Hartfield, and Elinor was offering her own doing as she could, and Elinor was in a flutter of spirits which was all to blame. She was vexed and alarmed; but she had scarcely opened her eyes from the maid who was standing at her seat; and Elinor, who had the advantage of Hunsford in the midst of her own humble love, and who had still the mortification of being useful to them, now found herself in a state of duty, to give the lie in her \n",
      "Emma sat thinking about him too; and Mrs. Jennings, now to be two years of his own age, found her gloom and liveliness when she spoke--for which she felt her friend to be happy, she could not be very uneasy when she told her Miss Bennet before, and it was too late to receive an answer. It was too long ago to be conveying her thoughts of her own family again, when she could be introduced, than Miss Bates looked at her sister, and Mrs. Weston was almost as welcome to urge \n",
      "Emma sat thinking about her husbands lodgings. Jane looked at her daughter's book, her books, and all the usual pain of the world. \"You may have no notion of it,\" he cried; \"it is quite as insignificant as a post in Westminster. What am there to prevent him to marry? Who does you give you leave to persuade me to introduce her?\" \"I was in love with you on the spot where I had seen. I hoped he must have staid three days to be married when he first were so \n",
      "Emma sat thinking about her, and they walked about and followed, the first to be done by some one, to enjoy a friend at once; perhaps the party being walking together in a quietness very considerable, provided Frank Churchill had gained the way that had failed a year; the result of all that was that she might not think of so, but she said no more. It was soon better understood, and as she hesitated, by no means till that _she_ had gone beyond the house. Nor was it of a single nature to be \n",
      "samples produced...\n",
      "garbage collected...\n",
      "session cleared (to save memory)...\n",
      "epoch:  25\n",
      "245/245 [==============================] - 26s 103ms/step - loss: 1.4712\n",
      "finished training...\n",
      "Emma sat thinking about her sister and Mrs. Jennings only wanted to be a great deal better than usual. She felt that she had been so much more pleased with every thing to be endured, and her own exertions were so much more gay and independent of the others as they were now, that she had been within a moment a few minutes before he could have done for her sister's decease. She had never been able to speak of Miss Darcy, but she had not been able to say how to class her from her aunt\n",
      "Emma sat thinking about her friend. She had not been able to part with her friends, and she was as grateful to her own heart as to be happy herself. The first sentence of her praise was a great drawback; and the intercourse of her present visit had been in great contentment with the utmost kindness. She was a little disappointed in her good offices, and her own exertions to do her good by the knowledge of her present choice, and the assiduous unhappiness which her situation had placed him in her opinion of herself and her sister. \n",
      "Emma sat thinking about her brother-in-law, and asked him if she had not been so happy as to entertain Mrs. Elton a letter from Hartfield. The Miss Steeles, Mr. Perry, Mrs. Jennings, a very respectable vicar. I suppose I was to call on Mrs. Goddards in the evening. The Mitchells intended _her_ by the entrance of Lucy Steele, and she was inquiring about me. I shall certainly have gone to Bath in the same room with Jane Fairfax. I am bewildered when \n",
      "Emma sat thinking about her, and she joined the card and passing on the hot stairs, which she had obtained some time before, and saying, in a sarcastic manner, \"And where is Fanny? Is this all a beautiful young woman?\" \"Yes, yes, then, and when that is to be done with you, I shall have this very day.\" \"And Edmund!--I hope she is only just like Maple Grove. And her figure begins in a very disagreeable air, and _that_ is what I told you all \n",
      "Emma sat thinking about them all in her deep mortification. Mr. Knightley was worked in. She found that he had written to Mrs. Jennings, and believed Mrs. Jennings by her very mother' death, and was coming two hours before she was able to attend her son to Mrs. Goddards in the evening. Mrs. Weston wished he might carry him thither, and Fanny was perfectly satisfied--and Mrs. Weston was not satisfied. The chief of the two friends were again in the same room, and Catherine could not withhold\n",
      "Emma sat thinking about the badness of every projected evening, and for their being so crowded between the group, &c. which must suit him as nearly beyond their heads, was nothing still beyond the neighbourhood, without any reluctance from friendship or pain, and from those who loved many years they gave them. It was a all Emma to expect for her sentiments to Mr. Dixon, and the kindness of a king on which he had no right idea of his companions; and yet could Sir Walter look as he went by surprise and working, his\n",
      "samples produced...\n",
      "garbage collected...\n",
      "session cleared (to save memory)...\n",
      "epoch:  26\n",
      "245/245 [==============================] - 26s 103ms/step - loss: 1.3094\n",
      "finished training...\n",
      "Emma sat thinking about her in the same way of Mr. Eltons being in Bath. He had been sitting with her on business at Netherfield, and had been sitting with her mother, and with her mother, and one of the officers, but she had been a little unwell  , a gentleman and a great favourite with her, and Jane was in hysterics being left for a few minutes without saying a word, and Mrs. Jennings was obliged to go down to the Parsonage herself. The two ladies had set out together. The \n",
      "Emma sat thinking about it before. She had heard her cry while she waited in her walk. She felt it was not in the matter of sight; for it was a very awkward business, and then he was assured of Mr. Eltons having gone through her walk with Mr. Wickham. Mrs. Jennings had gone to her brother; and he had met with the Thorpes and Mr. Weston in her good humour and good luck, and in the course of the following conversation, he had told her that he had been sitting with \n",
      "Emma sat thinking about the whole day which had been prepared to be complimented.--She was in high spirits, and was wretched. Her father had been kept waiting at least with her family, but knew it was not for Mr Charles's great expectations of good-will. Mr. Knightley had been at Bath only since she had a letter from Richmond, and her warm curiosity was to follow the Palmers, the good understanding of the fashionable world was soon before her usual knowledge of Mr. Darcys having been in Bath, and the \n",
      "Emma sat thinking about it in her disturbed and painful humiliation; and whenever she spoke, she thought she saw it all, and lament over the first act which Elinor had received. The visit, therefore, were by no means so favourable to her. The day was so happily settled, that she was to dine with her in Bartlett's Buildings; and when Mr. Collins had joined her on the street to Lady Middleton, she had already had time to go with her husband, and Mr. and Mrs. Collins, though at no \n",
      "Emma sat thinking about the difference between Bingley and herself, which must not be pardoned the whole of the evening. It was his public endeavour to find that her mother was only able to get on the wedding after the best dinner that might reasonably come from her; but she soon began again, and Elinor was again watching from her mothers dressing room, and began to remember her telling her how she went on long before she stopped to call her letter, and gave her an invitation an early day with her very good opinion of Mrs. Jennings\n",
      "Emma sat thinking about the part of this slow, minute of every painted paleness which had lain asleep. They were still much more sanguine and more hysterical than she had ever known them: four days had people in general. They were no means of driving the path; it was pronounced to be Charlotte, and Colonel Brandon talked of Lyme that would be more surprising to him. Mrs Croft and Mr. Wickham, and his mother, and Charles soon after who reached Pemberley, had done some of his fortune without any good grace. \n",
      "samples produced...\n",
      "garbage collected...\n",
      "session cleared (to save memory)...\n",
      "epoch:  27\n",
      "245/245 [==============================] - 26s 103ms/step - loss: 1.1483\n",
      "finished training...\n",
      "Emma sat thinking about her so warmly, and was miserable. The journey was dreaded by the excessive folly of her mother and Kitty. The Tilneys were to go with her to Clifton, and cease to be happy again. The two ladies were a remarkably fine family, and the squire of Captain Harville's being in the North. The Miss Dashwoods were all in ecstasies. Anne had a great deal to think of; and Elizabeth had no share in the festivities of the season, and she was very eager to be invited to\n",
      "Emma sat thinking about him again, and was beginning to arrange by the instinctive unconcern of her sister never to hear of her displeasure; but Elinor felt that she had never been so happy in the mildness of all the rest she mixed in the world. The resolution of seeming so very grateful did her in her opinion of Lucy's conduct, and still more of her own vanity that could induce him to overcome her own influence from Edward's inclination. The event was not very far from it. It was her intention of accepting her brother\n",
      "Emma sat thinking about him. She had not been more used to company with the connection than by her subsequent illness, and he did not think it right; but as she considered that Mr. Darcy must be the doer of it, she could not be mistaken. She would not appear to think of her for many hours. It was the greatest comfort I took my oath. I think I have said that he could not be staying with me in Hertfordshire, but I never was the widow to marry her extremely. I should \n",
      "Emma sat thinking about her several minutes in safety. She was sure she could be so handsome as herself. Harriet was the first of the two who had seen the performances, and sometimes in her own mind was actually the height of a shop. She was obliged to destroy the effect of unexpected solicitude for Miss Bates, in which she was now beginning to lead her in to the Great House party. They were not very comfortable, but the smiles and saucers had been left in the gipsies. It was too awkward; and she was\n",
      "Emma sat thinking about them all, till she felt for her sister-in-law what it would be. It was an animating change; and Emma never opened the engagement, as she expressed herself more anxious even at home, as to her friends attachment at Rosings. \"It was a great deal too much for her not to be sensible of the ill-looking sailor, who was all my ambition and consequence, the most fearful of the mischief--at least she was at, in the height of his own family--the only woman in Bath that might \n",
      "Emma sat thinking about him better than ever, declared she must always find her companion looking very much too much attached to her fortune, than she had received none since she had written from the house. She had a notion, when nobody else appeared. But his usual inclination for getting away at Netherfield, every thing was comfortable. That was her home, for any engagements but other young lady is unhappy.\" \"All this,\" cried Emma, \"seemed to be a disinterested welcome. Perhaps that I should welcome love from Frank Churchill, she \n",
      "samples produced...\n",
      "garbage collected...\n",
      "session cleared (to save memory)...\n",
      "epoch:  28\n",
      "245/245 [==============================] - 26s 103ms/step - loss: 0.9953\n",
      "finished training...\n",
      "Emma sat thinking about the whole party in the drawing-room to Mrs. Goddards. Her uncle and aunt were by no means forgotten in the house at Netherfield, and was now in lodgings at Longbourn. The Palmers were speaking of Mr. Woodhouse, who was only a very fine boy, and talked the year after the Miss Bertrams going to London. He had been at Hartfield only four miles from Rosings, and Jane had taken a liking to the ball. Mrs. Jennings and Kitty and Lydia were together together,\n",
      "Emma sat thinking about the whole party, and took her room and repeatedly into the house. Mr. Collins was the first to announce the inquiries which were still animated, and was not at all surprised to hear him say how much he was to be shown through the dining parlour, and therefore left them with the most wretched sensations. She was not very much astonished, but as if she had no objection, and his being in such cottage would have been duly given to the house, or whether he were to be the chosen companion, and whether \n",
      "Emma sat thinking about the whole letter and spoken of her mother, and Jane herself in the gallery. She knew that they had been at Hartfield the next day, and been very much pleased with the cousins. The young mans being to be at least always the finest young women in the room for Hartfield; Jane was not aware of it; but Mrs. Allen had a little curiosity to see Mrs. Jennings, Miss Tilney, and all the other among her servants. Chapter Xiv It was the first time since Miss \n",
      "Emma sat thinking about the whole letter and the same with Mr. Knightley. She was then interrupted by Lucy in a few moments to Mrs. Weston, and he had taken down a chair behind the group, and trying to understand what she did, and have left the house. It was the only time she could speak. It was an awkward, but not a moment from Frank Churchill, who seemed almost determined to get away. But Lady Middleton would go, she would go home. They were settled together, but Mr.\n",
      "Emma sat thinking about the next, and her pen were in the middle of two hours before the others left them. They were in a very unsettled state of rain that evening. It was only a triumphant exchange party; but Catherine could not listen to him. Mr. Darcy stepped forward to say, to his wishes of speaking as he could then acknowledge, with an air of grave reflection pushed the colour into his face. The ejaculation in his countenance was great. She looked at him slightly, and was composed and grave; and this was \n",
      "Emma sat thinking about him, and her dying had nothing to do. \"If youll read worse than anybody else,\" said Mrs. Weston, \"to let William and Her wife to themselves every year, will give the other to Wimpole Street, and suppose her _wishes_ _among_ her concerns and promoting her notice of Sir Thomas. No wonder she should say that she was a good girl; in vain did Lady Bertram smile and make her sit on the sofa with herself and pug, and vain was even the sight of immediately \n",
      "samples produced...\n",
      "garbage collected...\n",
      "session cleared (to save memory)...\n",
      "epoch:  29\n",
      "245/245 [==============================] - 26s 103ms/step - loss: 0.8518\n",
      "finished training...\n",
      "Emma sat thinking about the others in an instant. She was very low--but she could not bear to see her look so well as she had been used to be. She was not at leisure, she supposed, but she was _more_ attached to him; and the consciousness of being asked for an opportunity was never to be lost. She was miserable for the sake of having loved her. She was ashamed to conceal the beginning of the separation, and her being in sad scenes would be the governing truth with the possession of her mind,\n",
      "Emma sat thinking about the whole of the mornings at Lyme. She had to be in Her house, she was the sweetest and blooming creature, with rich and muffin. She had the age of tyranny, and was grieved to be her only companion. The Tilneys came back again to her charge; and she was then glad to be employed by her, without any danger of seeing Edward at all; and the usual purpose of ridiculing the newcomers. A very few minutes more spent in London was spent in this change by the day before.\n",
      "Emma sat thinking about it at a more space time than she could bear to see the expression of her complexion;--she could not but feel that she was not suffering from any thing strange. It would be an interesting, light-sized mind for him by chance, whenever to be good, and the coarseness of her face; and she could have interrupted her even to the last half arrived--Elizabeth agreed at paying her compliments immediately. Mr. Bingley had been at the Parsonage yesterday. He had dined at the Parsonage only the preceding day. \n",
      "Emma sat thinking about the whole of the morning at Rosings; but as Louisa maintained it to them all, and till they parted, Anne was preparing for herself to receive a seat, and, at the end of a week, set off to join them. Chapter 27 The next day brought Catherines good principles on the subject of that moral elevation which she felt on seeing Janes cousins. Her first remark was of course so very great a interest to give time to make him certain that he was as amiable as her sister's, and \n",
      "Emma sat thinking about her father with tolerable calmness, and again glancing towards him on the subject of her hand, and protesting himself of his satisfaction by her careless to her sister. \"Oh! yes, you can't carry everybody happy in it,\" he replied; \"I do not know whether it _is_ so; and besides, I think we never shall.\" \"I was determined to get up and see the place; and I knew that I should not have a key to anything--and that would now be the same. So it \n",
      "Emma sat thinking about the end of the bench; and she found it was forced to be diverted, as she thought he would now take up the party.--Since she had called upon her excellent friends, she was very willing to make herself agreeable, and she was too ill to interest herself. How could it be done?--How wonderfully all this she had begun, what could she say to Marianne? the change, his gaiety, her expressions, her gentle reproach; it was transient to know whether she had done more; happy to escape the face\n",
      "samples produced...\n",
      "garbage collected...\n",
      "session cleared (to save memory)...\n"
     ]
    }
   ],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "model.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "num_epochs_total = 30\n",
    "if restart:\n",
    "  start_epoch = 0\n",
    "else:\n",
    "  start_epoch = epoch_to_pickup\n",
    "for e in range(start_epoch, num_epochs_total):\n",
    "  success = False\n",
    "  while(success == False):\n",
    "    try:\n",
    "      print(\"epoch: \", e)\n",
    "      if e < num_epochs_total-10:\n",
    "        new_text = getRandomText(numbooks = 20)\n",
    "      else:\n",
    "        new_text = sherlock_text + getRandomText(numbooks = (num_epochs_total - e)//10)\n",
    "      # new_text = getMyText(files)\n",
    "      dataset = text_to_dataset(new_text)\n",
    "      del new_text\n",
    "      dataset = setup_dataset(dataset)\n",
    "      #opt = tf.keras.optimizers.Adam(learning_rate=0.002*(0.97**e))\n",
    "      #model.compile(optimizer=opt, loss=loss)\n",
    "      model.optimizer.learning_rate.assign(0.002*(0.99**e))\n",
    "      model.fit(dataset, epochs=1, verbose=1)\n",
    "      print(\"finished training...\")\n",
    "      del dataset\n",
    "      #print(\"saving weights...\")\n",
    "      #model.save_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")\n",
    "      #print(\"weights saved...\")\n",
    "      if(e > num_epochs_total-10):\n",
    "        for temp in [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "          produce_sample(model,vectorize_layer,vocabulary, temp, e, 'Emma sat thinking about')\n",
    "        print(\"samples produced...\")\n",
    "        gc.collect()\n",
    "        print(\"garbage collected...\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        print(\"session cleared (to save memory)...\")\n",
    "        #tf.config.experimental.reset_all()\n",
    "      success = True\n",
    "    except:\n",
    "      gc.collect()\n",
    "      tf.keras.backend.clear_session()\n",
    "      #tf.config.experimental.reset_all()\n",
    "      try:\n",
    "        del dataset\n",
    "      except:\n",
    "        print(\"dataset already deleted\")\n",
    "      print(\"retrying epoch: \" , e)\n",
    "\n",
    "# Send training complete notification \n",
    "notification.notify(\n",
    "    title='Training Complete!',\n",
    "    message = 'Done!',\n",
    "    app_icon=None,\n",
    "    timeout=10,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RNN_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
