{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aUB4xrFdLkr8"
      },
      "outputs": [],
      "source": [
        "restart = True\n",
        "epoch_to_pickup = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XtiXE04uGB_U"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import contextlib\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "import gc  # Import the garbage collector module\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUgiww4oQ75T",
        "outputId": "329d5de7-4197-45e3-e5a4-76acde4a9c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ],
      "source": [
        "# Get a list of GPU devices\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "    try:\n",
        "        # Enable memory growth for all GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        \n",
        "        # After enabling memory growth, we need to make sure TensorFlow sees the updated configuration\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(f\"Error setting memory growth: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "irakMtGnaImf"
      },
      "outputs": [],
      "source": [
        "path = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nDl6_okDOUyY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# path = '/content/drive/My Drive/M6_Fall2023e/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv4r-dKnSRKz"
      },
      "source": [
        "## Functions for downloading text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xzLUaBa2Xmnb"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    text = text.replace(\"Project Gutenberg\", \"\")\n",
        "    text = text.replace(\"Gutenberg\", \"\")\n",
        "\n",
        "    # Remove carriage returns\n",
        "    text = text.replace(\"\\r\", \"\")\n",
        "\n",
        "    # fix quotes\n",
        "    text = text.replace(\"“\", \"\\\"\")\n",
        "    text = text.replace(\"”\", \"\\\"\")\n",
        "\n",
        "    # Replace any capital letter at the start of a word with ^ followed by the lowercase letter\n",
        "    text = re.sub(r\"(?<![a-zA-Z])([A-Z])\", lambda match: f\"^{match.group(0).lower()}\", text)\n",
        "\n",
        "    # Replace all other capital letters with lowercase\n",
        "    text = re.sub(r\"([A-Z])\", lambda match: f\"{match.group(0).lower()}\", text)\n",
        "\n",
        "    # Remove duplicate whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
        "    text = re.sub(r\"\\t+\", \"\\t\", text)\n",
        "\n",
        "    # Replace whitespace characters with special words\n",
        "    text = re.sub(r\"(\\t)\", r\" zztabzz \", text)\n",
        "    text = re.sub(r\"(\\n)\", r\" zznewlinezz \", text)\n",
        "    text = re.sub(r\"(\\s)\", r\" zzspacezz \", text)\n",
        "\n",
        "    # Split before and after punctuation\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, f\" {punctuation} \")\n",
        "\n",
        "    # Makes sure that any non utf-8 characters don't make it through\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nFKehxVF9AxD"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(text):\n",
        "\n",
        "    # Replace special words with whitespace characters\n",
        "    text = text.replace(\"zztabzz\", \"\\t\")\n",
        "    text = text.replace(\"zznewlinezz\", \"\\n\")\n",
        "    text = text.replace(\"zzspacezz\", \" \")\n",
        "\n",
        "    # Remake capital letters at beginning of words\n",
        "    text = re.sub(r\"\\^([a-z])\", lambda match: f\"{match.group(1).upper()}\", text)\n",
        "\n",
        "    text = text.replace(\"^\", \"\")\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Makes sure that any non utf-8 characters don't make it through\n",
        "def normalize_text(text):\n",
        "\n",
        "    # Normalize to remove accents and special characters\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtd9QyvUWqzi"
      },
      "source": [
        "# def getMyText():\n",
        "#   path_to_file = tf.keras.utils.get_file('austen.txt', 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt')\n",
        "\n",
        "#   text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "#   # path_to_file = tf.keras.utils.get_file('903-0.txt', 'https://www.gutenberg.org/files/903/903-0.txt')\n",
        "#   # author_text += open(path_to_file, 'rb').read().decode(encoding='utf-8')[2999:-19194]\n",
        "#   # tf.io.gfile.remove(path_to_file)\n",
        "\n",
        "#   return preprocess_text(text)\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def getMyText():\n",
        "    file_name = 'austen.txt'\n",
        "    file_url = 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt'\n",
        "    local_dir = './data/'  # Directory to save the file\n",
        "    local_path = os.path.join(local_dir, file_name)\n",
        "\n",
        "    try:\n",
        "        # Ensure the directory exists\n",
        "        if not os.path.exists(local_dir):\n",
        "            os.makedirs(local_dir)\n",
        "\n",
        "        # Check if the file exists locally\n",
        "        if os.path.exists(local_path):\n",
        "            print(f\"File '{file_name}' found locally. Using it.\")\n",
        "        else:\n",
        "            print(f\"File '{file_name}' not found locally. Downloading it.\")\n",
        "            # Download the file\n",
        "            downloaded_path = tf.keras.utils.get_file(file_name, file_url)\n",
        "\n",
        "            # Save the downloaded file to the designated local directory\n",
        "            with open(downloaded_path, 'rb') as source_file:\n",
        "                with open(local_path, 'wb') as dest_file:\n",
        "                    dest_file.write(source_file.read())\n",
        "\n",
        "        # Read the file's contents\n",
        "        with open(local_path, 'rb') as file:\n",
        "            text = file.read().decode(encoding='utf-8')\n",
        "\n",
        "        return preprocess_text(text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shakespeare text import\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def getMyText():\n",
        "    file_name = 'shakespeare.txt'\n",
        "    file_url = 'https://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
        "    local_dir = './data/'  # Directory to save the file\n",
        "    local_path = os.path.join(local_dir, file_name)\n",
        "\n",
        "    try:\n",
        "        # Ensure the directory exists\n",
        "        if not os.path.exists(local_dir):\n",
        "            os.makedirs(local_dir)\n",
        "\n",
        "        # Check if the file exists locally\n",
        "        if os.path.exists(local_path):\n",
        "            print(f\"File '{file_name}' found locally. Using it.\")\n",
        "        else:\n",
        "            print(f\"File '{file_name}' not found locally. Downloading it.\")\n",
        "            # Download the file\n",
        "            downloaded_path = tf.keras.utils.get_file(file_name, file_url)\n",
        "\n",
        "            # Save the downloaded file to the designated local directory\n",
        "            with open(downloaded_path, 'rb') as source_file:\n",
        "                with open(local_path, 'wb') as dest_file:\n",
        "                    dest_file.write(source_file.read())\n",
        "\n",
        "        # Read the file's contents\n",
        "        with open(local_path, 'rb') as file:\n",
        "            text = file.read().decode(encoding='utf-8')\n",
        "\n",
        "        return preprocess_text(text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "rofy7hJ1iHVm",
        "outputId": "c0eaceda-482d-47a6-c54a-10c40d7aeaea"
      },
      "source": [
        "getMyText()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gsCd-ihOU02C"
      },
      "outputs": [],
      "source": [
        "def getRandomText(numbooks = 1, verbose=False):\n",
        "  # Create a buffer to capture download output messages\n",
        "  download_log = io.StringIO()\n",
        "  # Initialize empty string to store text from books\n",
        "  text_random = ''\n",
        "  # Loop through the requested number of books\n",
        "  for b in range(numbooks):\n",
        "    # Flag to track whether we've found a suitable book\n",
        "    foundbook = False\n",
        "    # Keep trying until we find a valid book\n",
        "    while(foundbook == False):\n",
        "      # Generate random book ID between 100-60000\n",
        "      booknum = random.randint(100,60000)\n",
        "      if verbose:\n",
        "        print('Trying Book #: ',booknum)\n",
        "      # Randomly choose between two URL formats for Project Gutenberg\n",
        "      if random.random() > 0.5:\n",
        "        url = 'https://www.gutenberg.org/files/' + str(booknum) + '/' + str(booknum) + '-0.txt'\n",
        "        filename_temp = str(booknum) + '-0.txt'\n",
        "      else:\n",
        "        url = 'https://www.gutenberg.org/cache/epub/' + str(booknum) + '/pg' + str(booknum) + '.txt'\n",
        "        filename_temp = 'pg' + str(booknum) + '.txt'\n",
        "      if verbose:\n",
        "        print('Trying: ', url)\n",
        "      try:\n",
        "        # Download the file, either showing progress or hiding it based on verbose flag\n",
        "        if verbose:\n",
        "          path_to_file_temp = tf.keras.utils.get_file(filename_temp, url)\n",
        "        else:\n",
        "          with contextlib.redirect_stdout(download_log):\n",
        "            path_to_file_temp = tf.keras.utils.get_file(filename_temp, url)\n",
        "        # Read the downloaded file and decode as UTF-8\n",
        "        temptext = open(path_to_file_temp, 'rb').read().decode(encoding='utf-8')\n",
        "        # Delete the temporary file after reading its contents\n",
        "        tf.io.gfile.remove(path_to_file_temp)\n",
        "        # Check if the book is in English\n",
        "        if (temptext.find('Language: English') >= 0):\n",
        "          # Add slight randomness to the starting position\n",
        "          offset = random.randint(-20,20)\n",
        "          # Skip the header/preamble text (typically first 2000 chars)\n",
        "          header = 2000\n",
        "          # Target length for extracted text\n",
        "          total_length = 200000\n",
        "          # Amount to trim from the end of the book\n",
        "          chopoffend = 10000\n",
        "          # For long books: extract a fixed-size chunk\n",
        "          if len(temptext) > (header+total_length+offset+chopoffend):\n",
        "            foundbook = True\n",
        "            text_random += temptext[header+offset:header+total_length+offset]\n",
        "            #print(\"Yes: \" + str(booknum))\n",
        "            if verbose:\n",
        "              print('New size of dataset: ', len(text_random))\n",
        "          # For medium-length books: take most of the book minus header and footer\n",
        "          elif len(temptext) > (header+12000):\n",
        "            foundbook = True\n",
        "            text_random += temptext[header:-chopoffend]\n",
        "            #print(\"Yes (smaller): \" + str(booknum))\n",
        "            if verbose:\n",
        "              print('New size of dataset: ', len(text_random))\n",
        "          # Skip books that are too short\n",
        "          else:\n",
        "            if verbose:\n",
        "              print('Not long enough. Trying again...')\n",
        "            #print(\"No: \" + str(booknum) + \" too short\")\n",
        "        # Skip non-English books\n",
        "        else:\n",
        "          if verbose:\n",
        "            print('Not English. Trying again...')\n",
        "          #print(\"No: \" + str(booknum) + \" not English\")\n",
        "        # Clean up memory\n",
        "        del temptext\n",
        "      # Handle any exceptions during download/processing\n",
        "      except:\n",
        "        if verbose:\n",
        "          print('Not valid file. Trying again...')\n",
        "        #print(\"No: \" + str(booknum) + \" not valid\")\n",
        "        foundbook = False\n",
        "    if verbose:\n",
        "      print(\"Found \" + str(b+1) + \" books so far...\")\n",
        "  # Clean up memory\n",
        "  del download_log\n",
        "  # The following commented code appears to be for further processing that was replaced\n",
        "  #text_random = \"\".join(c for c in text_random if c in vocab)\n",
        "  #all_ids_random = ids_from_chars(tf.strings.unicode_split(text_random, 'UTF-8'))\n",
        "  #ids_dataset_random = tf.data.Dataset.from_tensor_slices(all_ids_random)\n",
        "  #sequences_random = ids_dataset_random.batch(seq_length+1, drop_remainder=True)\n",
        "  #dataset_random = sequences_random.map(split_input_target)\n",
        "  #dataset_random = (dataset_random.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "  #return dataset_random\n",
        "  \n",
        "  # Pass the collected text to a preprocessing function and return the result\n",
        "  return preprocess_text(text_random)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjEF0LKxhljS",
        "outputId": "c0e93e56-0db7-40b3-db3c-1aae811aed42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'shakespeare.txt' found locally. Using it.\n"
          ]
        }
      ],
      "source": [
        "if restart:\n",
        "  vocab_text = getMyText()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpFvtyF_g3jY"
      },
      "source": [
        "Make vocabulary (Adapted from TensorFlow word embedding tutorial)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "F8E6Q6dkMEpd"
      },
      "outputs": [],
      "source": [
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size = 8192\n",
        "sequence_length = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AWXUqLQ6g3KB"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_string):\n",
        "    # Lowercase the text\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    \n",
        "    # Handle common punctuation marks one by one\n",
        "    # This avoids regex escaping issues\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'\\.', ' . ')  # Period\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r',', ' , ')   # Comma\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'!', ' ! ')   # Exclamation\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'\\?', ' ? ')  # Question mark\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r';', ' ; ')   # Semicolon\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r':', ' : ')   # Colon\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'\\(', ' ( ')  # Open parenthesis\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'\\)', ' ) ')  # Close parenthesis\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'\\[', ' [ ')  # Open bracket\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'\\]', ' ] ')  # Close bracket\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'\\{', ' { ')  # Open brace\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'\\}', ' } ')  # Close brace\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'\"', ' \" ')   # Double quote\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r\"'\", \" ' \")   # Single quote\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'-', ' - ')   # Hyphen\n",
        "    lowercase = tf.strings.regex_replace(lowercase, r'\\*', '')   # Hyphen\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    return tf.strings.regex_replace(lowercase, r'\\s+', ' ')\n",
        "\n",
        "if restart:\n",
        "  # Use the text vectorization layer to normalize, split, and map strings to\n",
        "  # integers. Note that the layer uses the custom standardization defined above.\n",
        "  # Set maximum_sequence length as all samples are not of the same length.\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize=custom_standardization,\n",
        "      split='whitespace',\n",
        "      max_tokens=vocab_size,\n",
        "      output_mode='int',\n",
        "      #output_sequence_length=sequence_length\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zJfr5w1bTWiJ"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "  vectorize_layer.adapt([vocab_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PmaoiyvF1Ilm"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocabulary = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ULNtM_8nYn"
      },
      "source": [
        "Save Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G1hjxv447INt"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  with open(path + \"vocabulary.txt\", \"w\") as file:\n",
        "    for word in vocabulary:\n",
        "        file.write(word + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7qn5MjC8p0_"
      },
      "source": [
        "Load Saved Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TLbSoqUP8Pxu"
      },
      "outputs": [],
      "source": [
        "if restart == False:\n",
        "  with open(path + \"vocabulary.txt\", \"r\") as file:\n",
        "      vocabulary = [word.strip() for word in file.readlines()]\n",
        "      vocabulary = vocabulary\n",
        "\n",
        "  vectorize_layer = TextVectorization(\n",
        "      vocabulary=vocabulary,\n",
        "      standardize='lower',\n",
        "      split='whitespace',\n",
        "      max_tokens=vocab_size,\n",
        "      output_mode='int',\n",
        "      #output_sequence_length=sequence_length\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FidGlurb1iD3",
        "outputId": "50b05e9e-68b7-4cda-a427-b879a3e3a5b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['', '[UNK]', 'zzspacezz', '^', ',', '.', 'the', 'and', 'i', 'to', 'of', 'a', 'you', 'my', ';', 'in', 'that', '?', 'is', '_']\n",
            "['englishmen', 'engenders', 'enfranchisement', 'encounters', 'enchanted', 'empery', 'embracd', 'embassage', 'elbows', 'eighth', 'echoes', 'ebbs', 'earn', 'dusky', 'dulcet', 'ducat', 'drug', 'drowns', 'drench', 'dreamd']\n"
          ]
        }
      ],
      "source": [
        "print(vocabulary[:20])\n",
        "print(vocabulary[-20:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LovypAGk91Yp"
      },
      "source": [
        "Turn text into a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Mnp0huUX93Wi"
      },
      "outputs": [],
      "source": [
        "# This function will generate our sequence pairs:\n",
        "def split_input_target(sequence):\n",
        "    input_ids = sequence[:-1]\n",
        "    target_ids = sequence[1:]\n",
        "    return input_ids, target_ids\n",
        "\n",
        "# This function will create the dataset\n",
        "def text_to_dataset(text):\n",
        "  all_ids = vectorize_layer(text)\n",
        "  ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "  del all_ids\n",
        "  sequences = ids_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "  del ids_dataset\n",
        "\n",
        "  # Call the function for every sequence in our list to create a new dataset\n",
        "  # of input->target pairs\n",
        "  dataset = sequences.map(split_input_target)\n",
        "  del sequences\n",
        "\n",
        "  # shuffle\n",
        "\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afRybxef_QHi"
      },
      "source": [
        "Test on vocab text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0tBa6ttN_Ufz"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocab_ds = text_to_dataset(vocab_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vq191mRgWv2w"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  text = ''.join([vocabulary[index] for index in ids])\n",
        "  return postprocess_text(text)\n",
        "\n",
        "vocabulary_adjusted = vocabulary\n",
        "vocabulary_adjusted[0] = '[UNK]'\n",
        "vocabulary_adjusted[1] = ''\n",
        "\n",
        "words_from_ids = StringLookup(vocabulary=vocabulary_adjusted, invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDqaTHXFAEBD",
        "outputId": "7dc76f61-3d65-4d2a-c8ca-6ff765949c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: \n",
            "tf.Tensor(\n",
            "[   3    6    2 5044    2   10    2    3    6    2    3 3322    2    3\n",
            " 1502    2   10    2    3 1184    2    3 6155    2    3   28    2 5044\n",
            "    2   18    2   24    2    6    2  319    2   10    2 4671    2 6467\n",
            "    2   15    2    6    2    3 3568    2    3 2346    2    7    2  121\n",
            "    2  173    2  895    2   10    2    6    2  189    2   65    2   45\n",
            "    2 1814    2    7    2   21    2  666    2   45    2    1    2 4100\n",
            "    5    2    3   12    2   93    2 3231    2   23    4    2  112    2\n",
            "   23    2  147    2   67    2 1061   47  319    2   23    2  361    2\n",
            "    6    2 1049    2   10    2    6    2    3 3209    2    1    2   21\n",
            "    2   28], shape=(128,), dtype=int64)\n",
            "The ebook of The Complete Works of William Shakespeare This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no  whatsoever. You may copy it, give it away or re-use it under the terms of the License  with this\n",
            "tf.Tensor(\n",
            "[b'^' b'the' b'zzspacezz' b'ebook' b'zzspacezz' b'of' b'zzspacezz' b'^'\n",
            " b'the' b'zzspacezz' b'^' b'complete' b'zzspacezz' b'^' b'works'\n",
            " b'zzspacezz' b'of' b'zzspacezz' b'^' b'william' b'zzspacezz' b'^'\n",
            " b'shakespeare' b'zzspacezz' b'^' b'this' b'zzspacezz' b'ebook'\n",
            " b'zzspacezz' b'is' b'zzspacezz' b'for' b'zzspacezz' b'the' b'zzspacezz'\n",
            " b'use' b'zzspacezz' b'of' b'zzspacezz' b'anyone' b'zzspacezz' b'anywhere'\n",
            " b'zzspacezz' b'in' b'zzspacezz' b'the' b'zzspacezz' b'^' b'united'\n",
            " b'zzspacezz' b'^' b'states' b'zzspacezz' b'and' b'zzspacezz' b'most'\n",
            " b'zzspacezz' b'other' b'zzspacezz' b'parts' b'zzspacezz' b'of'\n",
            " b'zzspacezz' b'the' b'zzspacezz' b'world' b'zzspacezz' b'at' b'zzspacezz'\n",
            " b'no' b'zzspacezz' b'cost' b'zzspacezz' b'and' b'zzspacezz' b'with'\n",
            " b'zzspacezz' b'almost' b'zzspacezz' b'no' b'zzspacezz' b'' b'zzspacezz'\n",
            " b'whatsoever' b'.' b'zzspacezz' b'^' b'you' b'zzspacezz' b'may'\n",
            " b'zzspacezz' b'copy' b'zzspacezz' b'it' b',' b'zzspacezz' b'give'\n",
            " b'zzspacezz' b'it' b'zzspacezz' b'away' b'zzspacezz' b'or' b'zzspacezz'\n",
            " b're' b'-' b'use' b'zzspacezz' b'it' b'zzspacezz' b'under' b'zzspacezz'\n",
            " b'the' b'zzspacezz' b'terms' b'zzspacezz' b'of' b'zzspacezz' b'the'\n",
            " b'zzspacezz' b'^' b'license' b'zzspacezz' b'' b'zzspacezz' b'with'\n",
            " b'zzspacezz' b'this'], shape=(128,), dtype=string)\n",
            "Target: \n",
            "tf.Tensor(\n",
            "[   6    2 5044    2   10    2    3    6    2    3 3322    2    3 1502\n",
            "    2   10    2    3 1184    2    3 6155    2    3   28    2 5044    2\n",
            "   18    2   24    2    6    2  319    2   10    2 4671    2 6467    2\n",
            "   15    2    6    2    3 3568    2    3 2346    2    7    2  121    2\n",
            "  173    2  895    2   10    2    6    2  189    2   65    2   45    2\n",
            " 1814    2    7    2   21    2  666    2   45    2    1    2 4100    5\n",
            "    2    3   12    2   93    2 3231    2   23    4    2  112    2   23\n",
            "    2  147    2   67    2 1061   47  319    2   23    2  361    2    6\n",
            "    2 1049    2   10    2    6    2    3 3209    2    1    2   21    2\n",
            "   28    2], shape=(128,), dtype=int64)\n",
            "the ebook of The Complete Works of William Shakespeare This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no  whatsoever. You may copy it, give it away or re-use it under the terms of the License  with this \n"
          ]
        }
      ],
      "source": [
        "if restart:\n",
        "  for input_example, target_example in vocab_ds.take(1):\n",
        "    print(\"Input: \")\n",
        "    print(input_example)\n",
        "    print(text_from_ids(input_example))\n",
        "    print(words_from_ids(input_example))\n",
        "    print(\"Target: \")\n",
        "    print(target_example)\n",
        "    print(text_from_ids(target_example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Rp402vgrS54t"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "def setup_dataset(dataset):\n",
        "  dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "  return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0LdoMfT7T8WN"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocab_ds = setup_dataset(vocab_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VQ-KjEeZMzd"
      },
      "source": [
        "## III. Build the model\n",
        "\n",
        "Next, we'll build our model. Up until this point, you've been using the Keras symbolic, or imperative API for creating your models. Doing something like:\n",
        "\n",
        "    model = tf.keras.models.Sequentla()\n",
        "    model.add(tf.keras.layers.Dense(80, activation='relu))\n",
        "    etc...\n",
        "\n",
        "However, tensorflow has another way to build models called the Functional API, which gives us a lot more control over what happens inside the model. You can read more about [the differences and when to use each here](https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html).\n",
        "\n",
        "We'll use the functional API for our RNN in this example. This will involve defining our model as a custom subclass of `tf.keras.Model`.\n",
        "\n",
        "If you're not familiar with classes in python, you might want to review [this quick tutorial](https://www.w3schools.com/python/python_classes.asp), as well as [this one on class inheritance](https://www.w3schools.com/python/python_inheritance.asp).\n",
        "\n",
        "Using a functional model is important for our situation because we're not just training it to predict a single character for a single sequence, but as we make predictions with it, we need it to remember those predictions as use that memory as it makes new predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Fj4uh9y-Y9mx"
      },
      "outputs": [],
      "source": [
        "# Create our custom model. Given a sequence of characters, this\n",
        "# model's job is to predict what character should come next.\n",
        "class AustenTextModel(tf.keras.Model):\n",
        "\n",
        "  # This is our class constructor method, it will be executed when\n",
        "  # we first create an instance of the class\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__()\n",
        "\n",
        "    # Our model will have three layers:\n",
        "\n",
        "    # 1. An embedding layer that handles the encoding of our vocabulary into\n",
        "    #    a vector of values suitable for a neural network\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # 2. A GRU layer that handles the \"memory\" aspects of our RNN. If you're\n",
        "    #    wondering why we use GRU instead of LSTM, and whether LSTM is better,\n",
        "    #    take a look at this article: https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm\n",
        "    #    then consider trying out LSTM instead (or in addition to!)\n",
        "    #self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm1 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm2 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm3 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    #self.lstm4 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "\n",
        "\n",
        "    self.hidden1 = tf.keras.layers.Dense(embedding_dim*64, activation='relu')\n",
        "    self.hidden2 = tf.keras.layers.Dense(embedding_dim*16, activation='relu')\n",
        "    #self.hidden3 = tf.keras.layers.Dense(embedding_dim*4, activation='relu')\n",
        "\n",
        "    # 3. Our output layer that will give us a set of probabilities for each\n",
        "    #    character in our vocabulary.\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # This function will be executed for each epoch of our training. Here\n",
        "  # we will manually feed information from one layer of our network to the\n",
        "  # next.\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "\n",
        "    # 1. Feed the inputs into the embedding layer, and tell it if we are\n",
        "    #    training or predicting\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    # 2. If we don't have any state in memory yet, get the initial random state\n",
        "    #    from our GRUI layer.\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "    if states is None:\n",
        "      states1 = [tf.zeros([batch_size, self.lstm1.units]), tf.zeros([batch_size, self.lstm1.units])]\n",
        "      states2 = [tf.zeros([batch_size, self.lstm2.units]), tf.zeros([batch_size, self.lstm2.units])]\n",
        "      states3 = [tf.zeros([batch_size, self.lstm3.units]), tf.zeros([batch_size, self.lstm3.units])]\n",
        "      #states4 = [tf.zeros([batch_size, self.lstm4.units]), tf.zeros([batch_size, self.lstm4.units])]\n",
        "    else:\n",
        "      states1 = states[0]\n",
        "      states2 = states[1]\n",
        "      states3 = states[2]\n",
        "      #states4 = states[3]\n",
        "    # 3. Now, feed the vectorized input along with the current state of memory\n",
        "    #    into the gru layer.\n",
        "    x, state_h_1, state_c_1 = self.lstm1(x, initial_state=states1, training=training)\n",
        "    states_out_1 = [state_h_1,state_c_1]\n",
        "\n",
        "    x, state_h_2, state_c_2 = self.lstm2(x, initial_state=states2, training=training)\n",
        "    states_out_2 = [state_h_2,state_c_2]\n",
        "\n",
        "    x, state_h_3, state_c_3 = self.lstm3(x, initial_state=states3, training=training)\n",
        "    states_out_3 = [state_h_3,state_c_3]\n",
        "\n",
        "    #x, state_h_4, state_c_4 = self.lstm4(x, initial_state=states4, training=training)\n",
        "    #states_out_4 = [state_h_4,state_c_4]\n",
        "\n",
        "    states_out = [states_out_1, states_out_2, states_out_3]#, states_out_4]\n",
        "    #states_out = [states_out_1, states_out_2]\n",
        "\n",
        "    x = self.hidden1(x,training=training)\n",
        "    x = self.hidden2(x,training=training)\n",
        "    #x = self.hidden3(x,training=training)\n",
        "    # 4. Finally, pass the results on to the dense layer\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    # 5. Return the results\n",
        "    if return_state:\n",
        "      return x, states_out\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NGm9o_J8Tq2F"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  dataset = vocab_ds\n",
        "  del vocab_text\n",
        "  del vocab_ds\n",
        "else:\n",
        "  new_text = getRandomText(numbooks = 10)\n",
        "  dataset = text_to_dataset(new_text)\n",
        "  del new_text\n",
        "  dataset = setup_dataset(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UA2C6pxZc4De"
      },
      "outputs": [],
      "source": [
        "# Create an instance of our model\n",
        "#vocab_size=len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 128\n",
        "rnn_units = 512\n",
        "\n",
        "model = AustenTextModel(vocab_size, embedding_dim, rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C67kN7YAdfSf",
        "outputId": "87af41fd-6cbe-4d7b-9ff9-2cdce14a280d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 128, 8192) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Verify the output of our model is correct by running one sample through\n",
        "# This will also compile the model for us. This step will take a bit.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "qJGL8gCWdsiu",
        "outputId": "fe8a8959-c379-4369-83dc-22dba8d4e3d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"austen_text_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  1048576   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  multiple                  1312768   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                multiple                  2099200   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                multiple                  2099200   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  4202496   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  16779264  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  16785408  \n",
            "=================================================================\n",
            "Total params: 44,326,912\n",
            "Trainable params: 44,326,912\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Now let's view the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UDbtrI9tc2NH"
      },
      "outputs": [],
      "source": [
        "# Here's the code we'll use to sample for us. It has some extra steps to apply\n",
        "# the temperature to the distribution, and to make sure we don't get empty\n",
        "# characters in our text. Most importantly, it will keep track of our model\n",
        "# state for us.\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, vectorize_layer, vocabulary, temperature=1):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.vectorize_layer = vectorize_layer\n",
        "    self.vocabulary = vocabulary\n",
        "    #print(\"initialized\")\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = StringLookup(vocabulary=list(vocabulary))(['', '[UNK]'])[:, None]\n",
        "    #print(skip_ids)\n",
        "    #print(\"3\")\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(vocabulary)])\n",
        "    #print(\"4\")\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
        "    #print(\"5\")\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    #input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.vectorize_layer(inputs)\n",
        "    #print(input_ids)\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    del input_ids\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    del predicted_logits\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    #print(predicted_ids[0])\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return words_from_ids(predicted_ids), states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "P3WQoFaE7Ol2"
      },
      "outputs": [],
      "source": [
        "def produce_sample(model, vectorize_layer, vocabulary, temp, epoch, prompt):\n",
        "  # Create an instance of the character generator\n",
        "  #print(\"entered\")\n",
        "  one_step_model = OneStep(model, vectorize_layer, vocabulary, temp)\n",
        "  #print(\"rand one step\")\n",
        "  # Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
        "  # as its starting text\n",
        "  states = None\n",
        "  next_char = tf.constant([preprocess_text(prompt)])\n",
        "  result = [tf.constant([prompt])]\n",
        "\n",
        "  for n in range(200):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    #print(next_char)\n",
        "    result.append(next_char)\n",
        "    #print(result)\n",
        "\n",
        "  result = tf.strings.join(result)\n",
        "  #print(result)\n",
        "\n",
        "  # Print the results formatted.\n",
        "  #print('Temp: ' + str(temp) + '\\n')\n",
        "  print(postprocess_text(result[0].numpy().decode('utf-8')))\n",
        "  #print('\\n\\n')\n",
        "  print('Epoch: ' + str(epoch) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  print('Temp: ' + str(temp) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  print(postprocess_text(result[0].numpy().decode('utf-8')), file=open(path + 'tree.txt', 'a'))\n",
        "  print('\\n\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  del states\n",
        "  del next_char\n",
        "  del result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTDe5m4baEqo"
      },
      "source": [
        "## IV. Train the model\n",
        "\n",
        "For our purposes, we'll be using [categorical cross entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) as our loss function*. Also, our model will be outputting [\"logits\" rather than normalized probabilities](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow), because we'll be doing further transformations on the output later.\n",
        "\n",
        "\n",
        "\\* Note that since our model deals with integer encoding rather than one-hot encoding, we'll specifically be using [sparse categorical cross entropy](https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mOP5s0SmIhUO"
      },
      "outputs": [],
      "source": [
        "# sherlock_text = getMyText()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xSk7HBJe_RZi"
      },
      "outputs": [],
      "source": [
        "if restart == False:\n",
        "  model.load_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vOxc7CkaGQB",
        "outputId": "5524a5d1-2723-421e-9d05-de13ff75ab0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:  0\n",
            "File 'shakespeare.txt' found locally. Using it.\n",
            "291/291 [==============================] - 29s 89ms/step - loss: 3.6665\n",
            "finished training...\n",
            "Emma sat thinking about it. I will not I not be to not a King That are a King That I will not I do be the Duke To the King Page Scene I of King Of King Scene I House Enter King Scene I Scene I Scene Iii. The King Scene I Scene I Scene Iv. I Scene Iii. A Room Scene I Scene Iii. A Room and King Palace And Act I Room\n",
            "Emma sat thinking about the honour of Rome. Ill be I would not be I would not a That the Highness I will I have a Lord I have hang the world And the King Of I Scene I Scene I Enter Iii Scene Iii. A Room Scene I Scene I. I I Scene I Enter Duke Scene I Scene Iii. I Scene I Scene A Palace Scene I of Servant Scene I Scene Ii. \n",
            "Emma sat thinking about thou daggers, I am be in a house Of the lady of the ears of King Macbeth. The King Valentine, England and Capulets, Gloucester Scene I Act I Palace Scene I of King, I Scene I Scene Iii. A house Scene Exeunt. Scene V. The Palace Scene I. Enter Lord Scene I Scene I Scene I Scene I Scene I Scene I Scene Iii. Iv. A Room in \n",
            "Emma sat thinking about thee I often be praise of the world of the fire of his intent of the man With a abhorrd Duke Scene Iii. I Margaret. Behind the York Caius, I ill Duke Love with Duke Scorns Trojan dumb, My love King, The King The King Of King. The King I Chamberlain. Timon. The Helens Enter King Page, Timon. Mistress William. Caesar. Douglas. I am do which to be the dull. Lucio. \n",
            "Emma sat thinking about the man. But haste came on she give not the honours and all so life And be the Room of Lawrence. Buckingham. We again this piece for his life with And found they should is To our brains of thine men of my life, And have all him. A is you will out again, and for sweeter; I had follow to that have That seen they nothing of Moor, Servilius, a And flowers never never That will it done To with him lives of her matter at\n",
            "Emma sat thinking about their lark. His king that had so tell my love on her tale. What for I am speak to t. By Syracuse. I will hit an commend for the most grief That must not will much is betray me at his passion wind sake As palamon i ears, Our behalf might not rage, rise? One than thou fortunes will are hear not still, And waste not tried. Anon, a people heavenly bed, methought me is it, a King She is never; grey Bolingbroke and no soon\n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  1\n",
            "File 'shakespeare.txt' found locally. Using it.\n",
            "291/291 [==============================] - 27s 89ms/step - loss: 2.6263\n",
            "finished training...\n",
            "Emma sat thinking about her Christian work, And I have not a King of France To King Of Lancaster I And I Would I could not And I be a Highness Than I would not lose you with it, And I am a King I am not I not not I will not you to I am so well I would not be here to be to the King Richard. I will not come of your Highness To York of King Edward Enter Antonio\n",
            "Emma sat thinking about their tale, And in the world of his wife As they seem to be the same In Duke of France To The King of England Of York Of France Of York Of France Of France Of Cyprus Gower Scene Ii. A Palace House Scene Iii. The Palace Scene Iv. The forest in Palace Scene Ii. The Palace Scene Ii. Rome. Before the Palace Scene Ii. The Palace Scene Ii. The\n",
            "Emma sat thinking about my lord. But we had your tongue To be apt. And all that be I would be so long. Clown. Give us your leave to prove the sun and soon good gentleman, he is the fan of fear, and we come to thee here and you have I do not bring the King That the little woman in England? I gave it to be the world to be in Duke, I have going from Cassio hath so strange And be that of your honour and the King \n",
            "Emma sat thinking about in your face. This I am so impatient Betwixt by such long for a love That can be your care. Prince. To the King I have a daughter too, And we are so hath something to the state, To hold the month of thee to our common Christian read to France his fair better bow, And I saw you to Thebes Of Wales and Gloucester Claudio. Second Justice. O Valeria, you have a day in your Grace. Miranda. I am well,\n",
            "Emma sat thinking about all honest. But I think I thought with his water and rash The death of the castle and young Mistress Page. But I heard you know to make him to her! Hamlet. What wast at this Highness have now but faint I prevent it that to call her you would be the great state of a master, his foot of his daughter, and let you hear our sons in mine own queen. I am here, I come, and it is I give the gods flames back up from\n",
            "Emma sat thinking about me, That will repent it must be as supper. O mine own life, the day that appoint him, For have know your child to be, I am Lance for I hear thee, and I know you, I shall find none along now, I have been lost and learn me. Alcibiades. O Gentleman\" those thou children? Falstaff. What\"What Lucentio this? Fare you, come here what thou her? Emilia. Whats so, tell thee; something nor a Duke is I thought. \n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  2\n",
            "File 'shakespeare.txt' found locally. Using it.\n",
            "291/291 [==============================] - 27s 89ms/step - loss: 2.4762\n",
            "finished training...\n",
            "Emma sat thinking about The King Is I have lost her in your Grace, And I am to the Duke Of York I To Sir Andrew Of Gloucester Lord King Richard Of Timon Lord Northumberland Of King Henry Of Attendant Lord Sir John Of Queen Richard Lord Sir Lord Sir John Lord Sir John I I Of Lord Of Friend Sir Thomas Sir John Sir John Sir John Sir John Of Gloucester Lord\n",
            "Emma sat thinking about That I may be as you for I do not I In my heart of Caesar and I am But I am not a prince. But I have no more than you will be but I was a little man in your heart to me a gentleman of the enemy. Sir, I am no more than the devil, and I come not to your son, and I am so much. I have been a man with this I have seen you to the gods I will I\n",
            "Emma sat thinking about The Nurse Scene Iii. A Room in the Widows house Scene Iii. The same. A room in the Dukes House Enter Clown Andronicus, Duke of Clarence and Sir Hugh Pericles Sir John Northumberland Of Sir Slender Doctor Madam Lord Of Lord Gloucester Of Sir Brook Valentine Father Captain Sir John Sir Percy Lord Of Justice Nurse Lord Falstaff Sir John A Lord Sir Did I make a Kings Soldiers\n",
            "Emma sat thinking about And some gallant I have been well Than then you have it in war; And I to speak his prayers, But too much I have but too much. York. I will not not I had welcome me the Duke Of York. A Lord I am sure of me. I pray you, sir, Ill look to you to dinner such a gentleman nor I am not. Benedick. Sir, indeed, I would not your change. Sampson. She does wish your mind, Sir\n",
            "Emma sat thinking about them in. The King hath lost him to the office Of my children, so would that I To fetch her head to thy sprite, And what I shall have so far I stand to a cause, and to me the Grace Did acquaint a humble vessel to her name, I am a years and that I shake him To one by her wife, with us laugh down. He cannot not be a younger man, Whose unkindness is so armd, And what his honour, whose pain is most\n",
            "Emma sat thinking about the ravens. It was no honour to with thy cause Of mankind of a wonder of me, Are in the boy to partly a and nature; Thou tis the dog that of the caves, She would confine but history of heart. I come not well. Othello. My lord, it will I feed. In her, said \"Faith! What stand do\"_. Remember the skirts of the force that go dead in the winter. Away. O, think your noble judgement Is that have been fellowship to\n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  3\n",
            "File 'shakespeare.txt' found locally. Using it.\n",
            "291/291 [==============================] - 27s 89ms/step - loss: 2.3750\n",
            "finished training...\n",
            "Emma sat thinking about The King is Englands enemy, And here the King is Plantagenet That I am not to the King Richard. In Lucrece I might be to thee And to my life and I will be done. Silvia. I am not the man of Romeo, And I am not not so much of God. My lord, I will not go to my hand. The King is I to Rome, I say I am so much I shall be in you That \n",
            "Emma sat thinking about us, And with the King of England, I have seen The Queen of London Scene I. A Room in Petruchios House. Scene Iii. A room in State Scene Iii. The same Camp in Athens Scene Ii. The same Scene Iv. A Camp in State in Timons house Enter King Henry, Somerset, Oxford, Reignier, Soldiers and Soldiers, and Attendants. King Edward. What is my lord? First Gentleman. My lord,\n",
            "Emma sat thinking about The King of Richard. When I shall be this king, And since I wish me to use I am not I must be not nor no prayers Than I have not a very other. But he Shall be a lady of that I am sick As to his friend. I am as wise That I have loved thee to my desires That I have no power of his hand As I thought but your renowned Jack, And send me your friend, and bring us up\n",
            "Emma sat thinking about us In the English Majesty and Winchester. And for the Lady I am gone with The King Lewis is himself to the King Richard I to my mistress Kent Claudio, And tell me to Englands pardon When I do for Caesar did forget you. If this I render you, I cry With that our own father will not work To be the greatest princes. The day I do be as a gentleman That he and Palamon was dead in us The Athenian\n",
            "Emma sat thinking about The tennis and a wicked kings and loose, And here alone in eyes of Venus and the gods. And what art thou then, my lord? How now, do you do my sword? Proteus. Thou dost have done For more why thou wouldst be. What is it this That is the Moor, I know you all, And Ill supply your trial. King John. And play oer the same appointed. But what should I perform with me? You are not mine wifes, York, \n",
            "Emma sat thinking about her since, You seem they that made none than a man it is, The adder has supper. The youth of my brother Is dangerous white and wrinkled distressed That could be crownd, but his nightly hands, And to what says eternal and a man, This thou hast rather one laid in thy skill. Look thou not, father, lest thy beast go thee, Tell these thou wilt have thou not awake, gentle night Nor in a happy secrecy a gods And give what like a men-beard still That \n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  4\n",
            "File 'shakespeare.txt' found locally. Using it.\n",
            "291/291 [==============================] - 27s 91ms/step - loss: 2.2904\n",
            "finished training...\n",
            "Emma sat thinking about the King of France And Duke of Gloucester Contents Act I Scene I. London. A Room in the Palace Scene Ii. A Room in Olivias House. Scene Ii. The same. A Room in the palace Enter Gower and Gentlemen Caius. Duke. How now, Sir Topas to Malvolio to him? Mistress Ford. Nay, I beseech you, sir, I am a gentleman to your lordship, and I have made you a woman of you.\n",
            "Emma sat thinking about him, And I will be Palamon to him again. King Edward. How now, I pray you with the Duke of York. His Grace of Buckingham, and there I To Emperor High Prince Of Fifth Contents Act I Scene I. London. A Street Scene Iii. A Room in Leonatos House. Enter Leonato and Leonato. Viola. Sir, sir, I will not speak to your daughter. Sir, I have the Duke of Wales \n",
            "Emma sat thinking about Your Grace and death of his trial with his friends, The Duke of Bourbon Scene I. Another part of Wales. Scene Ii. London. A Street. Enter King Henry and Gloucester. Gloucester. For Caesar hath done me wrong with me. York. My lord, my lord, give me your sovereign leisure. By books of lands and tokens to this war Which you shall effect to this word to say, I think not in this intelligence I have so much of my \n",
            "Emma sat thinking about the sea-house, And this virtue and the French General Keeps me upon us with his Council here Of worthy Diomed, and come to Caesar, And yet I must not have so much loose To be a gentleman of her love. Julia. I am sorry for you that I am made In the bosom that I should know me. I promise thee for, I am my lord, I fear She will be my back for my right sake. Romeo. Look on me to thy\n",
            "Emma sat thinking about him to his Prince Richard, and partly other Train Andronicus, his Gentlemen Ferdinand, a Sisters Old Berowne, wife to Antium King John of Milan Woodville, Ladies strikes him by the Duke of York, To Vaughan Earl of Wiltshire, whose ships Forces Rambures killed to a King of Goths. Gloucester. Then here I pray, come to your daughter again. He shall be third to speak and all your counsel. But I do not, and that I was too \n",
            "Emma sat thinking about her With hey a book? Very simple in The arm of Athens, crown these divorce of place, So but a thousand ducats in the guard. Other of my palace, O, you come pale. A goose is deceivd, a babe than trifles With sin from an aim to observe you, That my woeful self of thee is my love. Richard. Nay, how is that servant _-mercy that is the villain Which if thou dost meet in this device drawn Which it is to hate with Henry \n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  5\n",
            "File 'shakespeare.txt' found locally. Using it.\n",
            "291/291 [==============================] - 27s 91ms/step - loss: 2.2141\n",
            "finished training...\n",
            "Emma sat thinking about the Greeks, And with the rest of Rome and Hastings The Duke of Suffolk and Duke of York. Queen Elizabeth. And now I am I to be the King of York, And I have sent to France and Edward, Earl of Cambridge, Duke of York Duke of Gloucester Duke of York Duke of York of Norfolk Duke of York Duke of York Lord Grey Langley, Duke of York Duke of York Langley, Duke of\n",
            "Emma sat thinking about his eyes And set the King in Rome in England Is in my lord and Lord of York and Oxford, And all the rest of Bolingbroke and King Henry. Warwick. King Henry. O Warwick, Warwick is thy King Bolingbroke, Plantagenet, Queen Elizabeth, Queen Elizabeth to King Edward Duke Of Norfolk Duke of Norfolk Duke Of Norfolk Langley, son to King Henry Duke of York Lord of Richard Duke of York Elizabeth Duke \n",
            "Emma sat thinking about the world Of a poor and blasted that hath been a husband, And I had heard this service of her love, That I might be only of you so much That I am satisfied with my masters That I would have been a wise and bad In any love to the Duke of Pisa. Thurio. Gentlemen, I am glad you are here at full And I must not have you have me offended. But I have done Cassius love to you to Rome. King Henry\n",
            "Emma sat thinking about the other That is to be fought. But I have dreamed my part. I know not I have seen it or my to be That I am unworthy to my love. Clown. Who is the man? Rosalind. Ay, sir, if they are a very excellent wrong. Claudio. Nay, but you are deceived, let not be other words, Sir Toby. Page. Sir Toby. Nay, well, I am like to be hanged. Alas me gods, thou art a knave of the \n",
            "Emma sat thinking about What he of me shall be compare by that, That care so much can be not in my soul Than I put out of this. What shall be done, My lord, have I shall tell you from my life That send thy heart and ever I may say. Lucetta. Yes, not fate; for he is not so careful. I am in you, Lucentio, I would not miss. Margaret. I will be pinchd Milan, from a great presence, And had that children will be \n",
            "Emma sat thinking about his eyes To barbarous Tybalt, and doth crossd from thunder A lean and impatient and her lips that the mind Of unkindness. Who sall chides Palamon? Plague. Let it be damned; I will not be all: And she made you both without you should say, Since I had once blind he can think whether Your will so plead, nor woman too much. Enter Ariel. Pandarus. Whos here? Pandarus. What news is that I am important? What? Signior Gremio? Petruchio. \n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  6\n",
            "File 'shakespeare.txt' found locally. Using it.\n",
            "291/291 [==============================] - 27s 90ms/step - loss: 2.1400\n",
            "finished training...\n",
            "Emma sat thinking about the King As I would answer my father. King Henry. How now, Catesby, I do not be employed To set up in Andronicus and my lord. He is not sorry to be not in your love. But if you do not know me, I will prove The Prince of Lancaster. I have done To God I would have done to be admitted. King Richard. What, what is this? What is this strange That I have done the King and Edwards\n",
            "Emma sat thinking about a Christian And the French Captain Coriolanus The King of France And Duke Of York Sir William Stanley Sir William Sandys Duchess Of Venice Sir William William Lord William Page A Shepherd Servants Sir Robert Page Mistress Page Mistress Page, Sir John Hugh Evans Evans Gobbo, Sir Hugh Evans. Ford Sir John John Falstaff, Sir John John Falstaff, John Rugby, Claudio, Don John Anne Page,\n",
            "Emma sat thinking about the King The Two Scene I. A room in the Castle Scene Ii. Another part of the Field Act Iv Scene I. Westminster. A Room in the Boars Head Tavern House. Act V Scene I. A hall in Leonatos house. Enter Don John and Claudio. Escalus. Here is a day of the Duke of Lancaster, And I have met the Roman Roman And Princess, who in Venice of Berry Came by \n",
            "Emma sat thinking about Or cowardly one in the way of war, Which with his eyes a day be blasted hid, To be the thoughts that his usurping day Is thence in hot rage and side his post, And Talbot is alive with his captivity And with thy hand the Nevilles prince doth bear. King Richard. Why, then, my friends, what is the like to both? King Richard. I had rather prove to be so bold As I shall do him to the Duke of York. And to \n",
            "Emma sat thinking about the manner of his work, and in a new heels flying, and the very active dozen covered with the trademark license, with using any work as to be to a ballad as as the scenes-glass must have a smack of the Lord. I was a fool a beggar; and it is he so. Second Fisherman. But did he that come there against her closet? Hostess. O, did he do the straw? When was Signior Leonato? Fool. So to mad the news. Falstaff. His horses \n",
            "Emma sat thinking about the lark to revel To the Grecian ports, and having led Thy sharp humility and be tied from me, When youth shall fly to Thebes, broke upright, The commons learn our amorous and true men, To put my dread Sir John Fastolf Weep to you name to England; for I know Constantly not me that I am as to live, And by the meaning that I have had done. His fathers lords bears all these cruel war, That you whose brother which cannot he return \n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  7\n",
            "File 'shakespeare.txt' found locally. Using it.\n",
            "291/291 [==============================] - 27s 90ms/step - loss: 2.0664\n",
            "finished training...\n",
            "Emma sat thinking about The King of Naples, Thomas Mowbray, General of Tyre Sir Thomas Vaughan, Duke of Norfolk Sir Walter Herbert, Officers of King Henry, Queen of King Henry, Courtier, Nobleman of Tyre. King Henry. How now, Fluellen? Fluellen. I pray you, sir, I pray you, to be gone. Enter Mistress Quickly and Mistress Quickly. Mistress Page. Nay, I do not be so much as you say to be a marvellous good service.\n",
            "Emma sat thinking about the neck of Greece That thinks the boar of Rome. O, an old spectacle, The Duke of Naples and his Majesty, I do beseech you to my Grace of York, If I be dead, and I will make a sentence. Bolingbroke. Good morrow, Catesby, go to Richmond. Richard. I have no more deceived than thou art. Richard. Why, I will not be so much a thing Wherein I call the King of Naples. King Henry. But \n",
            "Emma sat thinking about the King. They have sent me to see a King; you were a king To take it at his fathers behalf To take my life to see my message to thee. Queen Elizabeth. What dost thou object now that love of York? Clifford. I have been up to know it is to breathe. Richard. And shall we send the King to see the Pope? Richard. The law that eer the Dauphin hath forsook thee. Richard. Now, sweet Clarence, I am sure of\n",
            "Emma sat thinking about his head, And with a merrier-toe had I in this end, And here the best is but a second men. On the next hour is thus, most courageous, The which my blood hath done his deeds to utter, Only to bring the Capitol to fill Our theme of Rome, and to arms with him Who after be possessed by English king, And in his fail I will not beg of him; But I will sweat into her crystal grave. Anne. Then am I to \n",
            "Emma sat thinking about the pit. Men have out their mouths and makes him better less Than twenty mortal man, which makes men note. Anon it was the cities of the country. Enter King Henry, attended; Salisbury and Forces the French Army. Cade. Captain Fluellen, the Constable Stafford, a Boy, a Captain, and Diana, Sir Hugh Peter, a Schoolmaster Margery, an old gentleman, Signior Gremio and Claudio, daughter, Twin, Orlando, and Officers Margery. Leonato. You are four\n",
            "Emma sat thinking about to black and following, With outward end he kissd the steep. But since your actions enjoy their lights, Look not not like the cordial. This must do Her prayers and fall and shame of you, Although it prove a gentleman for a friend. I am a soldier; therefore crush me to your pleasures To the invention lose. Antonio. Good my lord, I cannot call you any more good Now to your understanding and service, coz, You thank too willingly, and not a spacious ear Say too \n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  8\n",
            "File 'shakespeare.txt' found locally. Using it.\n",
            "291/291 [==============================] - 27s 90ms/step - loss: 1.9936\n",
            "finished training...\n",
            "Emma sat thinking about my brother A prince of Padua, and his sister I Have loved him well a little of his eye. King. I have heard him that you have done to France. Enter Stanley. Duke Frederick. What is the matter? Duke. He is a traitor. Duke. I have forgot it. Duke. I know him well. Katharine. I have no hope to see a good time. Petruchio. I will not fail me more than I am honest. Julia. I have a \n",
            "Emma sat thinking about a painter, And in his conscience he was I to call. This is the first that very noble Duke Hath sent to Padua to your daughter, And I will give him to attend the Duke, And she shall marry him. Duke. I know your Highness Is not the cause of me. King. I would it had been found To give a great wager to the King. First Senator. I hope the King is dead. Enter Gardiner. How now, Ophelia! \n",
            "Emma sat thinking about so weighty To Marss altar. You are welcome, And let your voices call us all again: We have not put our latest conference on. King Henry. I know the lady of my father Gloucester Did not be a traitor to the King. King. What can you think when you did do your part? Bolingbroke. Ay, or no, I have done a noble word in me. Hastings. I must make my pains to come with me And give a better satisfaction. Widow. My\n",
            "Emma sat thinking about the land Of the King himself, whose bearing complaint Is full of safety, and he is beset. Archbishop. O I have got a seal i th charge to him. Juliet. And what cause is done that for her breach Hath made the Duke to this ensuing death, And I am rightful heir to Englands wits. King Edward. A and Lavinia, now, Earl of Westmoreland? Prince Richard. I know your honour to be satisfied. The King of Hereford and England and\n",
            "Emma sat thinking about my bones, His memory still set this day in; And when we hear our lives, they shall perceive, Let him depart with us and echo spread. Duke Frederick. Thart now the sergeant-fox. Why, good Lucius; Ill see the Duke of Naples above, Else struck the Duchess, who unto the King. Come, let us hear each others, and prove This same Lord Hastings mad, and when he has By the main Duke. What says the crown? The Duke of \n",
            "Emma sat thinking about me both To kiss the angels themselves upon his grave. Yet call thou Muse whom Collatinus sits, And whiter with my gentle thoughts doth mean. Enter Demetrius. Lavinia. Speak softly with me; therefore, brother Lucius: Tell me how the good reason did you know To you whose youth, and well return tomorrow, And know how much the law was fully. Chief Justice. Is it such a plot in person, my lord? Duke. Ay, in me: I am glad of my speech else\n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  9\n",
            "File 'shakespeare.txt' found locally. Using it.\n",
            "291/291 [==============================] - 27s 90ms/step - loss: 1.9208\n",
            "finished training...\n",
            "Emma sat thinking about the Duke, And I will make his service to the Tower. And when I hear him in the Tower walls, To whom I am in Englands royal person. King Richard. Then I have done a thousand pound of heaven, And I will give him leave to meet him next. King Richard. Why then preferred you for my noble lord? King. What is the matter? Messenger. I am sorry that you have a son of France To be a worthy queen and \n",
            "Emma sat thinking about the King That guides his eyes with all the Greekish clime, To make my soul a kind of man to live. What say you that thou canst not show me more To blazon me forbearance? I am no less Than I have seen thee in thy tale, And yet thou seemst a Ethiope were alive. But I will not be so much in the world, For that my love is not so beautys faith. Ah, I am gentle to be counted fair, And I will leave \n",
            "Emma sat thinking about the Duke Of Ephesus. And the Duke of Lancaster and Duke Of Dorset Duke Of Norfolk Lord Hastings Lord Clifford Sir William Lucy Sir William Page Montgomery Lord Of Venice Sir Thomas Gargrave Sir James Sir Christopher Duke Of Venice Duke Of Arragon Captain Sicilian Clown Sicilian Clown Lucius, friend to Brabantio Lodovico, Kinsman to Brabantio Publius, friend to Brabantio Othello, Friend to Brabantio Othello, Othello, Roderigo \n",
            "Emma sat thinking about the horse To climb the wild rocks vault of death. I come to you, And raze this loathsome abject stroke Of twenty times, whose blood and mild show bred. And here thou diest, thy Duke will make thee more, And then thy praise, that thou hast marrd me. Lavinia. Yes, truly, I do give thee leave, And that I am an hour or wonderful Hath made me power to send him company. Good George, I shall have justice and my friend. Duke. God\n",
            "Emma sat thinking about the Prince, With brave Mercutio, the Beggar on the walls. King Philip. Great Richard! Duke, that hath a knee That holds thy daughter from me that I found, Be glad, no doubt, it is the fatal winds That you have used to see. Bolingbroke. I have forgot my crown; And then I boldly am at liberty. So much is mine to Englands Majesty That Harry Percy, this is Edwards practices, And given him conquered in the coronation, And \n",
            "Emma sat thinking about my grief, And at the contrary joy it rests. How many love should say, I love her deadly, Or thee to be the due of thy Rome; My parts hath sent unto the King, at Ephesus, And say, father dead. Saturninus. Now, peace, thou coward daughter, be for evil. [_To Theseus._] The mother of our drooping welkin, did repute The taste of sorrow and their griefs by fight. When great Sinon shalt be used, I found This tedious cordial \n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n"
          ]
        }
      ],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "model.compile(optimizer=opt, loss=loss)\n",
        "\n",
        "num_epochs_total = 10\n",
        "if restart:\n",
        "  start_epoch = 0\n",
        "else:\n",
        "  start_epoch = epoch_to_pickup\n",
        "for e in range(start_epoch, num_epochs_total):\n",
        "  success = False\n",
        "  while(success == False):\n",
        "    try:\n",
        "      print(\"epoch: \", e)\n",
        "      # if e < 50:\n",
        "      #   new_text = getRandomText(numbooks = 20)\n",
        "      # else:\n",
        "      #   new_text = sherlock_text + getRandomText(numbooks = (num_epochs_total - e)//10)\n",
        "      new_text = getMyText()\n",
        "      dataset = text_to_dataset(new_text)\n",
        "      del new_text\n",
        "      dataset = setup_dataset(dataset)\n",
        "      #opt = tf.keras.optimizers.Adam(learning_rate=0.002*(0.97**e))\n",
        "      #model.compile(optimizer=opt, loss=loss)\n",
        "      model.optimizer.learning_rate.assign(0.002*(0.99**e))\n",
        "      model.fit(dataset, epochs=1, verbose=1)\n",
        "      print(\"finished training...\")\n",
        "      del dataset\n",
        "      #print(\"saving weights...\")\n",
        "      #model.save_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")\n",
        "      #print(\"weights saved...\")\n",
        "      for temp in [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
        "        produce_sample(model,vectorize_layer,vocabulary, temp, e, 'Emma sat thinking about')\n",
        "      print(\"samples produced...\")\n",
        "      gc.collect()\n",
        "      print(\"garbage collected...\")\n",
        "      tf.keras.backend.clear_session()\n",
        "      print(\"session cleared (to save memory)...\")\n",
        "      #tf.config.experimental.reset_all()\n",
        "      success = True\n",
        "    except:\n",
        "      gc.collect()\n",
        "      tf.keras.backend.clear_session()\n",
        "      #tf.config.experimental.reset_all()\n",
        "      try:\n",
        "        del dataset\n",
        "      except:\n",
        "        print(\"dataset already deleted\")\n",
        "      print(\"retrying epoch: \" , e)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RNN_gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
